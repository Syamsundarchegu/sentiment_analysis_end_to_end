2025-02-13 12:48:21,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:48:21,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:48:21,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:48:21,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:50:53,504:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:50:53,504:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:50:53,504:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:50:53,504:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:52:20,071:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:52:20,072:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:52:20,072:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:52:20,072:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:54:43,951:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:54:43,951:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:54:43,951:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:54:43,951:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:55:21,070:INFO:PyCaret ClassificationExperiment
2025-02-13 12:55:21,070:INFO:Logging name: clf-default-name
2025-02-13 12:55:21,070:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-13 12:55:21,070:INFO:version 3.3.2
2025-02-13 12:55:21,070:INFO:Initializing setup()
2025-02-13 12:55:21,070:INFO:self.USI: 7ebd
2025-02-13 12:55:21,070:INFO:self._variable_keys: {'idx', 'X_train', 'gpu_n_jobs_param', 'y_train', 'logging_param', '_available_plots', 'fix_imbalance', 'y', 'seed', 'exp_id', 'X', 'fold_generator', 'USI', 'data', 'log_plots_param', '_ml_usecase', 'pipeline', 'is_multiclass', 'gpu_param', 'X_test', 'target_param', 'fold_shuffle_param', 'html_param', 'y_test', 'exp_name_log', 'n_jobs_param', 'memory', 'fold_groups_param'}
2025-02-13 12:55:21,070:INFO:Checking environment
2025-02-13 12:55:21,070:INFO:python_version: 3.10.15
2025-02-13 12:55:21,070:INFO:python_build: ('main', 'Oct  3 2024 07:22:19')
2025-02-13 12:55:21,070:INFO:machine: AMD64
2025-02-13 12:55:21,087:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-13 12:55:21,091:INFO:Memory: svmem(total=17008857088, available=5902106624, percent=65.3, used=11106750464, free=5902106624)
2025-02-13 12:55:21,091:INFO:Physical Core: 6
2025-02-13 12:55:21,091:INFO:Logical Core: 12
2025-02-13 12:55:21,091:INFO:Checking libraries
2025-02-13 12:55:21,091:INFO:System:
2025-02-13 12:55:21,091:INFO:    python: 3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]
2025-02-13 12:55:21,091:INFO:executable: C:\Users\syamc\anaconda3\envs\gen_ai\python.exe
2025-02-13 12:55:21,091:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-13 12:55:21,091:INFO:PyCaret required dependencies:
2025-02-13 12:55:21,116:INFO:                 pip: 24.2
2025-02-13 12:55:21,116:INFO:          setuptools: 75.1.0
2025-02-13 12:55:21,116:INFO:             pycaret: 3.3.2
2025-02-13 12:55:21,116:INFO:             IPython: 8.28.0
2025-02-13 12:55:21,116:INFO:          ipywidgets: 8.1.5
2025-02-13 12:55:21,116:INFO:                tqdm: 4.66.5
2025-02-13 12:55:21,116:INFO:               numpy: 1.26.4
2025-02-13 12:55:21,116:INFO:              pandas: 2.1.4
2025-02-13 12:55:21,116:INFO:              jinja2: 3.1.4
2025-02-13 12:55:21,116:INFO:               scipy: 1.11.4
2025-02-13 12:55:21,116:INFO:              joblib: 1.3.2
2025-02-13 12:55:21,116:INFO:             sklearn: 1.4.2
2025-02-13 12:55:21,116:INFO:                pyod: 2.0.3
2025-02-13 12:55:21,116:INFO:            imblearn: 0.13.0
2025-02-13 12:55:21,116:INFO:   category_encoders: 2.7.0
2025-02-13 12:55:21,117:INFO:            lightgbm: 4.5.0
2025-02-13 12:55:21,117:INFO:               numba: 0.61.0
2025-02-13 12:55:21,117:INFO:            requests: 2.32.3
2025-02-13 12:55:21,117:INFO:          matplotlib: 3.7.5
2025-02-13 12:55:21,117:INFO:          scikitplot: 0.3.7
2025-02-13 12:55:21,117:INFO:         yellowbrick: 1.5
2025-02-13 12:55:21,117:INFO:              plotly: 5.24.1
2025-02-13 12:55:21,117:INFO:    plotly-resampler: Not installed
2025-02-13 12:55:21,117:INFO:             kaleido: 0.2.1
2025-02-13 12:55:21,117:INFO:           schemdraw: 0.15
2025-02-13 12:55:21,117:INFO:         statsmodels: 0.14.4
2025-02-13 12:55:21,117:INFO:              sktime: 0.26.0
2025-02-13 12:55:21,117:INFO:               tbats: 1.1.3
2025-02-13 12:55:21,117:INFO:            pmdarima: 2.0.4
2025-02-13 12:55:21,117:INFO:              psutil: 6.0.0
2025-02-13 12:55:21,117:INFO:          markupsafe: 3.0.1
2025-02-13 12:55:21,117:INFO:             pickle5: Not installed
2025-02-13 12:55:21,117:INFO:         cloudpickle: 3.1.1
2025-02-13 12:55:21,117:INFO:         deprecation: 2.1.0
2025-02-13 12:55:21,117:INFO:              xxhash: 3.5.0
2025-02-13 12:55:21,117:INFO:           wurlitzer: Not installed
2025-02-13 12:55:21,117:INFO:PyCaret optional dependencies:
2025-02-13 12:55:21,887:INFO:                shap: Not installed
2025-02-13 12:55:21,887:INFO:           interpret: Not installed
2025-02-13 12:55:21,887:INFO:                umap: Not installed
2025-02-13 12:55:21,887:INFO:     ydata_profiling: Not installed
2025-02-13 12:55:21,887:INFO:  explainerdashboard: Not installed
2025-02-13 12:55:21,888:INFO:             autoviz: Not installed
2025-02-13 12:55:21,888:INFO:           fairlearn: Not installed
2025-02-13 12:55:21,888:INFO:          deepchecks: Not installed
2025-02-13 12:55:21,888:INFO:             xgboost: Not installed
2025-02-13 12:55:21,888:INFO:            catboost: Not installed
2025-02-13 12:55:21,888:INFO:              kmodes: Not installed
2025-02-13 12:55:21,888:INFO:             mlxtend: Not installed
2025-02-13 12:55:21,888:INFO:       statsforecast: Not installed
2025-02-13 12:55:21,888:INFO:        tune_sklearn: Not installed
2025-02-13 12:55:21,888:INFO:                 ray: Not installed
2025-02-13 12:55:21,888:INFO:            hyperopt: Not installed
2025-02-13 12:55:21,888:INFO:              optuna: Not installed
2025-02-13 12:55:21,888:INFO:               skopt: Not installed
2025-02-13 12:55:21,888:INFO:              mlflow: Not installed
2025-02-13 12:55:21,888:INFO:              gradio: Not installed
2025-02-13 12:55:21,888:INFO:             fastapi: 0.115.0
2025-02-13 12:55:21,888:INFO:             uvicorn: 0.31.1
2025-02-13 12:55:21,888:INFO:              m2cgen: Not installed
2025-02-13 12:55:21,888:INFO:           evidently: Not installed
2025-02-13 12:55:21,889:INFO:               fugue: Not installed
2025-02-13 12:55:21,889:INFO:           streamlit: Not installed
2025-02-13 12:55:21,889:INFO:             prophet: Not installed
2025-02-13 12:55:21,889:INFO:None
2025-02-13 12:55:21,889:INFO:Set up data.
2025-02-13 12:55:21,968:INFO:Set up folding strategy.
2025-02-13 12:55:21,968:INFO:Set up train/test split.
2025-02-13 12:55:22,021:INFO:Set up index.
2025-02-13 12:55:22,021:INFO:Assigning column types.
2025-02-13 12:55:22,025:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-13 12:55:22,068:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,072:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,105:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,105:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,147:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,148:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,174:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,174:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,174:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-13 12:55:22,217:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,243:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,244:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,286:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:55:22,314:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,314:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,314:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-13 12:55:22,385:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,385:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,456:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:22,459:INFO:Preparing preprocessing pipeline...
2025-02-13 12:55:22,461:INFO:Set up simple imputation.
2025-02-13 12:55:22,465:INFO:Set up encoding of categorical features.
2025-02-13 12:55:22,819:INFO:Finished creating preprocessing pipeline.
2025-02-13 12:55:22,825:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\syamc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=['...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Comments'],
                                    transformer=TargetEncoder(cols=['Comments'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-13 12:55:22,825:INFO:Creating final display dataframe.
2025-02-13 12:55:24,064:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (59744, 2)
4        Transformed data shape        (59744, 2)
5   Transformed train set shape        (41820, 2)
6    Transformed test set shape        (17924, 2)
7          Categorical features                 1
8      Rows with missing values              3.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              7ebd
2025-02-13 12:55:24,144:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:24,144:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:24,218:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:24,219:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:55:24,220:INFO:setup() successfully completed in 3.15s...............
2025-02-13 12:55:24,220:INFO:Initializing compare_models()
2025-02-13 12:55:24,220:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-13 12:55:24,220:INFO:Checking exceptions
2025-02-13 12:55:24,226:INFO:Preparing display monitor
2025-02-13 12:55:24,230:INFO:Initializing Logistic Regression
2025-02-13 12:55:24,230:INFO:Total runtime is 1.612504323323568e-05 minutes
2025-02-13 12:55:24,230:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:24,230:INFO:Initializing create_model()
2025-02-13 12:55:24,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:24,230:INFO:Checking exceptions
2025-02-13 12:55:24,230:INFO:Importing libraries
2025-02-13 12:55:24,230:INFO:Copying training dataset
2025-02-13 12:55:24,238:INFO:Defining folds
2025-02-13 12:55:24,238:INFO:Declaring metric variables
2025-02-13 12:55:24,238:INFO:Importing untrained model
2025-02-13 12:55:24,238:INFO:Logistic Regression Imported successfully
2025-02-13 12:55:24,239:INFO:Starting cross validation
2025-02-13 12:55:24,240:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:31,943:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,052:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,054:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,174:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,177:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,203:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,285:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,353:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,354:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,359:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:32,381:INFO:Calculating mean and std
2025-02-13 12:55:32,394:INFO:Creating metrics dataframe
2025-02-13 12:55:32,398:INFO:Uploading results into container
2025-02-13 12:55:32,399:INFO:Uploading model into container now
2025-02-13 12:55:32,400:INFO:_master_model_container: 1
2025-02-13 12:55:32,400:INFO:_display_container: 2
2025-02-13 12:55:32,400:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-13 12:55:32,401:INFO:create_model() successfully completed......................................
2025-02-13 12:55:32,591:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:32,591:INFO:Creating metrics dataframe
2025-02-13 12:55:32,595:INFO:Initializing K Neighbors Classifier
2025-02-13 12:55:32,595:INFO:Total runtime is 0.13942714929580688 minutes
2025-02-13 12:55:32,595:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:32,595:INFO:Initializing create_model()
2025-02-13 12:55:32,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:32,596:INFO:Checking exceptions
2025-02-13 12:55:32,596:INFO:Importing libraries
2025-02-13 12:55:32,596:INFO:Copying training dataset
2025-02-13 12:55:32,606:INFO:Defining folds
2025-02-13 12:55:32,606:INFO:Declaring metric variables
2025-02-13 12:55:32,607:INFO:Importing untrained model
2025-02-13 12:55:32,607:INFO:K Neighbors Classifier Imported successfully
2025-02-13 12:55:32,608:INFO:Starting cross validation
2025-02-13 12:55:32,609:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:37,180:INFO:Calculating mean and std
2025-02-13 12:55:37,181:INFO:Creating metrics dataframe
2025-02-13 12:55:37,183:INFO:Uploading results into container
2025-02-13 12:55:37,184:INFO:Uploading model into container now
2025-02-13 12:55:37,184:INFO:_master_model_container: 2
2025-02-13 12:55:37,184:INFO:_display_container: 2
2025-02-13 12:55:37,185:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-13 12:55:37,185:INFO:create_model() successfully completed......................................
2025-02-13 12:55:37,311:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:37,311:INFO:Creating metrics dataframe
2025-02-13 12:55:37,314:INFO:Initializing Naive Bayes
2025-02-13 12:55:37,314:INFO:Total runtime is 0.21808768113454183 minutes
2025-02-13 12:55:37,314:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:37,315:INFO:Initializing create_model()
2025-02-13 12:55:37,315:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:37,315:INFO:Checking exceptions
2025-02-13 12:55:37,315:INFO:Importing libraries
2025-02-13 12:55:37,315:INFO:Copying training dataset
2025-02-13 12:55:37,325:INFO:Defining folds
2025-02-13 12:55:37,325:INFO:Declaring metric variables
2025-02-13 12:55:37,325:INFO:Importing untrained model
2025-02-13 12:55:37,326:INFO:Naive Bayes Imported successfully
2025-02-13 12:55:37,326:INFO:Starting cross validation
2025-02-13 12:55:37,327:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:38,207:INFO:Calculating mean and std
2025-02-13 12:55:38,208:INFO:Creating metrics dataframe
2025-02-13 12:55:38,210:INFO:Uploading results into container
2025-02-13 12:55:38,210:INFO:Uploading model into container now
2025-02-13 12:55:38,211:INFO:_master_model_container: 3
2025-02-13 12:55:38,211:INFO:_display_container: 2
2025-02-13 12:55:38,211:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:55:38,211:INFO:create_model() successfully completed......................................
2025-02-13 12:55:38,328:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:38,329:INFO:Creating metrics dataframe
2025-02-13 12:55:38,331:INFO:Initializing Decision Tree Classifier
2025-02-13 12:55:38,332:INFO:Total runtime is 0.23505727450052896 minutes
2025-02-13 12:55:38,332:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:38,332:INFO:Initializing create_model()
2025-02-13 12:55:38,332:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:38,332:INFO:Checking exceptions
2025-02-13 12:55:38,332:INFO:Importing libraries
2025-02-13 12:55:38,332:INFO:Copying training dataset
2025-02-13 12:55:38,341:INFO:Defining folds
2025-02-13 12:55:38,341:INFO:Declaring metric variables
2025-02-13 12:55:38,341:INFO:Importing untrained model
2025-02-13 12:55:38,341:INFO:Decision Tree Classifier Imported successfully
2025-02-13 12:55:38,341:INFO:Starting cross validation
2025-02-13 12:55:38,343:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:39,219:INFO:Calculating mean and std
2025-02-13 12:55:39,220:INFO:Creating metrics dataframe
2025-02-13 12:55:39,222:INFO:Uploading results into container
2025-02-13 12:55:39,222:INFO:Uploading model into container now
2025-02-13 12:55:39,223:INFO:_master_model_container: 4
2025-02-13 12:55:39,223:INFO:_display_container: 2
2025-02-13 12:55:39,223:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-02-13 12:55:39,223:INFO:create_model() successfully completed......................................
2025-02-13 12:55:39,340:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:39,340:INFO:Creating metrics dataframe
2025-02-13 12:55:39,343:INFO:Initializing SVM - Linear Kernel
2025-02-13 12:55:39,343:INFO:Total runtime is 0.2518979509671529 minutes
2025-02-13 12:55:39,344:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:39,344:INFO:Initializing create_model()
2025-02-13 12:55:39,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:39,344:INFO:Checking exceptions
2025-02-13 12:55:39,344:INFO:Importing libraries
2025-02-13 12:55:39,344:INFO:Copying training dataset
2025-02-13 12:55:39,352:INFO:Defining folds
2025-02-13 12:55:39,352:INFO:Declaring metric variables
2025-02-13 12:55:39,352:INFO:Importing untrained model
2025-02-13 12:55:39,353:INFO:SVM - Linear Kernel Imported successfully
2025-02-13 12:55:39,353:INFO:Starting cross validation
2025-02-13 12:55:39,354:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:39,894:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:39,974:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,093:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,112:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,143:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,157:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,193:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,207:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,224:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,239:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,299:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,313:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,327:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,338:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,355:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,364:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,386:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,395:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:40,416:INFO:Calculating mean and std
2025-02-13 12:55:40,417:INFO:Creating metrics dataframe
2025-02-13 12:55:40,419:INFO:Uploading results into container
2025-02-13 12:55:40,420:INFO:Uploading model into container now
2025-02-13 12:55:40,420:INFO:_master_model_container: 5
2025-02-13 12:55:40,420:INFO:_display_container: 2
2025-02-13 12:55:40,421:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-13 12:55:40,421:INFO:create_model() successfully completed......................................
2025-02-13 12:55:40,543:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:40,543:INFO:Creating metrics dataframe
2025-02-13 12:55:40,547:INFO:Initializing Ridge Classifier
2025-02-13 12:55:40,547:INFO:Total runtime is 0.27197217543919877 minutes
2025-02-13 12:55:40,547:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:40,547:INFO:Initializing create_model()
2025-02-13 12:55:40,547:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:40,547:INFO:Checking exceptions
2025-02-13 12:55:40,547:INFO:Importing libraries
2025-02-13 12:55:40,548:INFO:Copying training dataset
2025-02-13 12:55:40,557:INFO:Defining folds
2025-02-13 12:55:40,557:INFO:Declaring metric variables
2025-02-13 12:55:40,557:INFO:Importing untrained model
2025-02-13 12:55:40,557:INFO:Ridge Classifier Imported successfully
2025-02-13 12:55:40,558:INFO:Starting cross validation
2025-02-13 12:55:40,559:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:40,977:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:40,992:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,073:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,090:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,091:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,107:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,179:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,191:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,293:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,296:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,305:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,306:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,335:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,344:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,404:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,411:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,415:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,423:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,430:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:41,437:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:55:41,460:INFO:Calculating mean and std
2025-02-13 12:55:41,461:INFO:Creating metrics dataframe
2025-02-13 12:55:41,463:INFO:Uploading results into container
2025-02-13 12:55:41,464:INFO:Uploading model into container now
2025-02-13 12:55:41,464:INFO:_master_model_container: 6
2025-02-13 12:55:41,464:INFO:_display_container: 2
2025-02-13 12:55:41,464:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-02-13 12:55:41,465:INFO:create_model() successfully completed......................................
2025-02-13 12:55:41,589:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:41,589:INFO:Creating metrics dataframe
2025-02-13 12:55:41,592:INFO:Initializing Random Forest Classifier
2025-02-13 12:55:41,592:INFO:Total runtime is 0.2893754839897155 minutes
2025-02-13 12:55:41,593:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:41,593:INFO:Initializing create_model()
2025-02-13 12:55:41,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:41,593:INFO:Checking exceptions
2025-02-13 12:55:41,593:INFO:Importing libraries
2025-02-13 12:55:41,593:INFO:Copying training dataset
2025-02-13 12:55:41,601:INFO:Defining folds
2025-02-13 12:55:41,601:INFO:Declaring metric variables
2025-02-13 12:55:41,601:INFO:Importing untrained model
2025-02-13 12:55:41,601:INFO:Random Forest Classifier Imported successfully
2025-02-13 12:55:41,602:INFO:Starting cross validation
2025-02-13 12:55:41,603:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:44,343:INFO:Calculating mean and std
2025-02-13 12:55:44,344:INFO:Creating metrics dataframe
2025-02-13 12:55:44,346:INFO:Uploading results into container
2025-02-13 12:55:44,347:INFO:Uploading model into container now
2025-02-13 12:55:44,347:INFO:_master_model_container: 7
2025-02-13 12:55:44,347:INFO:_display_container: 2
2025-02-13 12:55:44,347:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-02-13 12:55:44,347:INFO:create_model() successfully completed......................................
2025-02-13 12:55:44,471:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:44,472:INFO:Creating metrics dataframe
2025-02-13 12:55:44,475:INFO:Initializing Quadratic Discriminant Analysis
2025-02-13 12:55:44,475:INFO:Total runtime is 0.33744035164515174 minutes
2025-02-13 12:55:44,475:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:44,475:INFO:Initializing create_model()
2025-02-13 12:55:44,475:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:44,475:INFO:Checking exceptions
2025-02-13 12:55:44,475:INFO:Importing libraries
2025-02-13 12:55:44,475:INFO:Copying training dataset
2025-02-13 12:55:44,483:INFO:Defining folds
2025-02-13 12:55:44,484:INFO:Declaring metric variables
2025-02-13 12:55:44,484:INFO:Importing untrained model
2025-02-13 12:55:44,484:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-13 12:55:44,484:INFO:Starting cross validation
2025-02-13 12:55:44,485:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:44,901:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,009:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,068:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,191:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,196:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,239:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,287:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,311:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,388:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,394:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:45,419:INFO:Calculating mean and std
2025-02-13 12:55:45,420:INFO:Creating metrics dataframe
2025-02-13 12:55:45,424:INFO:Uploading results into container
2025-02-13 12:55:45,425:INFO:Uploading model into container now
2025-02-13 12:55:45,426:INFO:_master_model_container: 8
2025-02-13 12:55:45,426:INFO:_display_container: 2
2025-02-13 12:55:45,426:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-13 12:55:45,426:INFO:create_model() successfully completed......................................
2025-02-13 12:55:45,570:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:45,570:INFO:Creating metrics dataframe
2025-02-13 12:55:45,575:INFO:Initializing Ada Boost Classifier
2025-02-13 12:55:45,575:INFO:Total runtime is 0.35576451222101846 minutes
2025-02-13 12:55:45,576:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:45,576:INFO:Initializing create_model()
2025-02-13 12:55:45,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:45,576:INFO:Checking exceptions
2025-02-13 12:55:45,576:INFO:Importing libraries
2025-02-13 12:55:45,576:INFO:Copying training dataset
2025-02-13 12:55:45,589:INFO:Defining folds
2025-02-13 12:55:45,590:INFO:Declaring metric variables
2025-02-13 12:55:45,590:INFO:Importing untrained model
2025-02-13 12:55:45,591:INFO:Ada Boost Classifier Imported successfully
2025-02-13 12:55:45,591:INFO:Starting cross validation
2025-02-13 12:55:45,593:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:46,026:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,105:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,220:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,266:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,360:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,452:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,529:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,605:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,656:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:46,728:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:55:47,625:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:47,718:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:47,838:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:47,878:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:47,961:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,016:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,070:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,134:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,169:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,192:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:48,219:INFO:Calculating mean and std
2025-02-13 12:55:48,220:INFO:Creating metrics dataframe
2025-02-13 12:55:48,223:INFO:Uploading results into container
2025-02-13 12:55:48,223:INFO:Uploading model into container now
2025-02-13 12:55:48,224:INFO:_master_model_container: 9
2025-02-13 12:55:48,224:INFO:_display_container: 2
2025-02-13 12:55:48,224:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-02-13 12:55:48,224:INFO:create_model() successfully completed......................................
2025-02-13 12:55:48,361:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:48,361:INFO:Creating metrics dataframe
2025-02-13 12:55:48,364:INFO:Initializing Gradient Boosting Classifier
2025-02-13 12:55:48,364:INFO:Total runtime is 0.4022545297940572 minutes
2025-02-13 12:55:48,364:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:48,364:INFO:Initializing create_model()
2025-02-13 12:55:48,365:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:48,365:INFO:Checking exceptions
2025-02-13 12:55:48,365:INFO:Importing libraries
2025-02-13 12:55:48,365:INFO:Copying training dataset
2025-02-13 12:55:48,374:INFO:Defining folds
2025-02-13 12:55:48,374:INFO:Declaring metric variables
2025-02-13 12:55:48,374:INFO:Importing untrained model
2025-02-13 12:55:48,375:INFO:Gradient Boosting Classifier Imported successfully
2025-02-13 12:55:48,375:INFO:Starting cross validation
2025-02-13 12:55:48,376:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:58,132:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,220:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,293:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,379:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,394:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,430:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,451:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,460:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,484:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,548:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:58,569:INFO:Calculating mean and std
2025-02-13 12:55:58,570:INFO:Creating metrics dataframe
2025-02-13 12:55:58,572:INFO:Uploading results into container
2025-02-13 12:55:58,573:INFO:Uploading model into container now
2025-02-13 12:55:58,573:INFO:_master_model_container: 10
2025-02-13 12:55:58,573:INFO:_display_container: 2
2025-02-13 12:55:58,574:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-13 12:55:58,574:INFO:create_model() successfully completed......................................
2025-02-13 12:55:58,692:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:58,692:INFO:Creating metrics dataframe
2025-02-13 12:55:58,695:INFO:Initializing Linear Discriminant Analysis
2025-02-13 12:55:58,696:INFO:Total runtime is 0.5744417428970336 minutes
2025-02-13 12:55:58,696:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:58,696:INFO:Initializing create_model()
2025-02-13 12:55:58,696:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:58,696:INFO:Checking exceptions
2025-02-13 12:55:58,696:INFO:Importing libraries
2025-02-13 12:55:58,696:INFO:Copying training dataset
2025-02-13 12:55:58,707:INFO:Defining folds
2025-02-13 12:55:58,707:INFO:Declaring metric variables
2025-02-13 12:55:58,708:INFO:Importing untrained model
2025-02-13 12:55:58,708:INFO:Linear Discriminant Analysis Imported successfully
2025-02-13 12:55:58,708:INFO:Starting cross validation
2025-02-13 12:55:58,710:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:55:59,247:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,287:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,348:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,446:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,489:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,543:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,586:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,631:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,653:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,673:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:55:59,693:INFO:Calculating mean and std
2025-02-13 12:55:59,705:INFO:Creating metrics dataframe
2025-02-13 12:55:59,707:INFO:Uploading results into container
2025-02-13 12:55:59,707:INFO:Uploading model into container now
2025-02-13 12:55:59,708:INFO:_master_model_container: 11
2025-02-13 12:55:59,708:INFO:_display_container: 2
2025-02-13 12:55:59,708:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-13 12:55:59,708:INFO:create_model() successfully completed......................................
2025-02-13 12:55:59,826:INFO:SubProcess create_model() end ==================================
2025-02-13 12:55:59,827:INFO:Creating metrics dataframe
2025-02-13 12:55:59,829:INFO:Initializing Extra Trees Classifier
2025-02-13 12:55:59,830:INFO:Total runtime is 0.593344223499298 minutes
2025-02-13 12:55:59,830:INFO:SubProcess create_model() called ==================================
2025-02-13 12:55:59,830:INFO:Initializing create_model()
2025-02-13 12:55:59,830:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:55:59,830:INFO:Checking exceptions
2025-02-13 12:55:59,830:INFO:Importing libraries
2025-02-13 12:55:59,830:INFO:Copying training dataset
2025-02-13 12:55:59,839:INFO:Defining folds
2025-02-13 12:55:59,839:INFO:Declaring metric variables
2025-02-13 12:55:59,840:INFO:Importing untrained model
2025-02-13 12:55:59,840:INFO:Extra Trees Classifier Imported successfully
2025-02-13 12:55:59,840:INFO:Starting cross validation
2025-02-13 12:55:59,841:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:56:02,368:INFO:Calculating mean and std
2025-02-13 12:56:02,370:INFO:Creating metrics dataframe
2025-02-13 12:56:02,372:INFO:Uploading results into container
2025-02-13 12:56:02,373:INFO:Uploading model into container now
2025-02-13 12:56:02,374:INFO:_master_model_container: 12
2025-02-13 12:56:02,374:INFO:_display_container: 2
2025-02-13 12:56:02,374:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-02-13 12:56:02,374:INFO:create_model() successfully completed......................................
2025-02-13 12:56:02,507:INFO:SubProcess create_model() end ==================================
2025-02-13 12:56:02,507:INFO:Creating metrics dataframe
2025-02-13 12:56:02,510:INFO:Initializing Light Gradient Boosting Machine
2025-02-13 12:56:02,510:INFO:Total runtime is 0.6380149284998575 minutes
2025-02-13 12:56:02,510:INFO:SubProcess create_model() called ==================================
2025-02-13 12:56:02,511:INFO:Initializing create_model()
2025-02-13 12:56:02,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:56:02,511:INFO:Checking exceptions
2025-02-13 12:56:02,511:INFO:Importing libraries
2025-02-13 12:56:02,511:INFO:Copying training dataset
2025-02-13 12:56:02,520:INFO:Defining folds
2025-02-13 12:56:02,520:INFO:Declaring metric variables
2025-02-13 12:56:02,521:INFO:Importing untrained model
2025-02-13 12:56:02,521:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 12:56:02,521:INFO:Starting cross validation
2025-02-13 12:56:02,523:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:56:09,042:INFO:Calculating mean and std
2025-02-13 12:56:09,044:INFO:Creating metrics dataframe
2025-02-13 12:56:09,047:INFO:Uploading results into container
2025-02-13 12:56:09,048:INFO:Uploading model into container now
2025-02-13 12:56:09,049:INFO:_master_model_container: 13
2025-02-13 12:56:09,049:INFO:_display_container: 2
2025-02-13 12:56:09,050:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 12:56:09,050:INFO:create_model() successfully completed......................................
2025-02-13 12:56:09,217:INFO:SubProcess create_model() end ==================================
2025-02-13 12:56:09,217:INFO:Creating metrics dataframe
2025-02-13 12:56:09,221:INFO:Initializing Dummy Classifier
2025-02-13 12:56:09,221:INFO:Total runtime is 0.7498711347579956 minutes
2025-02-13 12:56:09,222:INFO:SubProcess create_model() called ==================================
2025-02-13 12:56:09,222:INFO:Initializing create_model()
2025-02-13 12:56:09,222:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F6F00CBB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:56:09,222:INFO:Checking exceptions
2025-02-13 12:56:09,222:INFO:Importing libraries
2025-02-13 12:56:09,222:INFO:Copying training dataset
2025-02-13 12:56:09,233:INFO:Defining folds
2025-02-13 12:56:09,240:INFO:Declaring metric variables
2025-02-13 12:56:09,240:INFO:Importing untrained model
2025-02-13 12:56:09,240:INFO:Dummy Classifier Imported successfully
2025-02-13 12:56:09,241:INFO:Starting cross validation
2025-02-13 12:56:09,242:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:56:09,758:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:09,853:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:09,913:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:09,982:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,023:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,064:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,087:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,118:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,137:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,146:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:56:10,162:INFO:Calculating mean and std
2025-02-13 12:56:10,165:INFO:Creating metrics dataframe
2025-02-13 12:56:10,169:INFO:Uploading results into container
2025-02-13 12:56:10,170:INFO:Uploading model into container now
2025-02-13 12:56:10,171:INFO:_master_model_container: 14
2025-02-13 12:56:10,171:INFO:_display_container: 2
2025-02-13 12:56:10,172:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-02-13 12:56:10,172:INFO:create_model() successfully completed......................................
2025-02-13 12:56:10,294:INFO:SubProcess create_model() end ==================================
2025-02-13 12:56:10,294:INFO:Creating metrics dataframe
2025-02-13 12:56:10,299:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-13 12:56:10,300:INFO:Initializing create_model()
2025-02-13 12:56:10,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:56:10,300:INFO:Checking exceptions
2025-02-13 12:56:10,301:INFO:Importing libraries
2025-02-13 12:56:10,301:INFO:Copying training dataset
2025-02-13 12:56:10,308:INFO:Defining folds
2025-02-13 12:56:10,308:INFO:Declaring metric variables
2025-02-13 12:56:10,308:INFO:Importing untrained model
2025-02-13 12:56:10,308:INFO:Declaring custom model
2025-02-13 12:56:10,309:INFO:Naive Bayes Imported successfully
2025-02-13 12:56:10,310:INFO:Cross validation set to False
2025-02-13 12:56:10,310:INFO:Fitting Model
2025-02-13 12:56:10,407:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:56:10,407:INFO:create_model() successfully completed......................................
2025-02-13 12:56:10,527:INFO:_master_model_container: 14
2025-02-13 12:56:10,527:INFO:_display_container: 2
2025-02-13 12:56:10,527:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:56:10,527:INFO:compare_models() successfully completed......................................
2025-02-13 12:56:10,527:INFO:Initializing tune_model()
2025-02-13 12:56:10,527:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>)
2025-02-13 12:56:10,527:INFO:Checking exceptions
2025-02-13 12:56:10,531:INFO:Copying training dataset
2025-02-13 12:56:10,536:INFO:Checking base model
2025-02-13 12:56:10,536:INFO:Base model : Naive Bayes
2025-02-13 12:56:10,537:INFO:Declaring metric variables
2025-02-13 12:56:10,537:INFO:Defining Hyperparameters
2025-02-13 12:56:10,662:INFO:Tuning with n_jobs=-1
2025-02-13 12:56:10,662:INFO:Initializing RandomizedSearchCV
2025-02-13 12:56:17,657:INFO:best_params: {'actual_estimator__var_smoothing': 2e-07}
2025-02-13 12:56:17,659:INFO:Hyperparameter search completed
2025-02-13 12:56:17,659:INFO:SubProcess create_model() called ==================================
2025-02-13 12:56:17,660:INFO:Initializing create_model()
2025-02-13 12:56:17,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019F72E3F730>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'var_smoothing': 2e-07})
2025-02-13 12:56:17,661:INFO:Checking exceptions
2025-02-13 12:56:17,661:INFO:Importing libraries
2025-02-13 12:56:17,661:INFO:Copying training dataset
2025-02-13 12:56:17,689:INFO:Defining folds
2025-02-13 12:56:17,689:INFO:Declaring metric variables
2025-02-13 12:56:17,690:INFO:Importing untrained model
2025-02-13 12:56:17,690:INFO:Declaring custom model
2025-02-13 12:56:17,691:INFO:Naive Bayes Imported successfully
2025-02-13 12:56:17,693:INFO:Starting cross validation
2025-02-13 12:56:17,696:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:56:18,589:INFO:Calculating mean and std
2025-02-13 12:56:18,590:INFO:Creating metrics dataframe
2025-02-13 12:56:18,592:INFO:Finalizing model
2025-02-13 12:56:18,731:INFO:Uploading results into container
2025-02-13 12:56:18,732:INFO:Uploading model into container now
2025-02-13 12:56:18,732:INFO:_master_model_container: 15
2025-02-13 12:56:18,732:INFO:_display_container: 3
2025-02-13 12:56:18,732:INFO:GaussianNB(priors=None, var_smoothing=2e-07)
2025-02-13 12:56:18,732:INFO:create_model() successfully completed......................................
2025-02-13 12:56:18,851:INFO:SubProcess create_model() end ==================================
2025-02-13 12:56:18,851:INFO:choose_better activated
2025-02-13 12:56:18,851:INFO:SubProcess create_model() called ==================================
2025-02-13 12:56:18,852:INFO:Initializing create_model()
2025-02-13 12:56:18,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019F6CBC3370>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:56:18,852:INFO:Checking exceptions
2025-02-13 12:56:18,852:INFO:Importing libraries
2025-02-13 12:56:18,853:INFO:Copying training dataset
2025-02-13 12:56:18,860:INFO:Defining folds
2025-02-13 12:56:18,860:INFO:Declaring metric variables
2025-02-13 12:56:18,860:INFO:Importing untrained model
2025-02-13 12:56:18,861:INFO:Declaring custom model
2025-02-13 12:56:18,861:INFO:Naive Bayes Imported successfully
2025-02-13 12:56:18,861:INFO:Starting cross validation
2025-02-13 12:56:18,862:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:56:19,692:INFO:Calculating mean and std
2025-02-13 12:56:19,693:INFO:Creating metrics dataframe
2025-02-13 12:56:19,695:INFO:Finalizing model
2025-02-13 12:56:19,828:INFO:Uploading results into container
2025-02-13 12:56:19,829:INFO:Uploading model into container now
2025-02-13 12:56:19,829:INFO:_master_model_container: 16
2025-02-13 12:56:19,829:INFO:_display_container: 4
2025-02-13 12:56:19,829:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:56:19,830:INFO:create_model() successfully completed......................................
2025-02-13 12:56:19,940:INFO:SubProcess create_model() end ==================================
2025-02-13 12:56:19,940:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.3813
2025-02-13 12:56:19,941:INFO:GaussianNB(priors=None, var_smoothing=2e-07) result for Accuracy is 0.3813
2025-02-13 12:56:19,941:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2025-02-13 12:56:19,941:INFO:choose_better completed
2025-02-13 12:56:19,941:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-02-13 12:56:19,952:INFO:_master_model_container: 16
2025-02-13 12:56:19,952:INFO:_display_container: 3
2025-02-13 12:56:19,952:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:56:19,952:INFO:tune_model() successfully completed......................................
2025-02-13 12:57:31,969:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:57:31,969:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:57:31,969:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:57:31,969:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 12:58:02,075:INFO:PyCaret ClassificationExperiment
2025-02-13 12:58:02,075:INFO:Logging name: clf-default-name
2025-02-13 12:58:02,075:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-13 12:58:02,075:INFO:version 3.3.2
2025-02-13 12:58:02,075:INFO:Initializing setup()
2025-02-13 12:58:02,075:INFO:self.USI: 35ed
2025-02-13 12:58:02,075:INFO:self._variable_keys: {'fold_generator', 'gpu_n_jobs_param', 'data', 'is_multiclass', 'memory', 'X_train', 'USI', 'fold_groups_param', '_available_plots', '_ml_usecase', 'target_param', 'gpu_param', 'pipeline', 'idx', 'seed', 'fold_shuffle_param', 'y_train', 'log_plots_param', 'html_param', 'fix_imbalance', 'logging_param', 'X', 'exp_id', 'X_test', 'n_jobs_param', 'y', 'exp_name_log', 'y_test'}
2025-02-13 12:58:02,075:INFO:Checking environment
2025-02-13 12:58:02,075:INFO:python_version: 3.10.15
2025-02-13 12:58:02,076:INFO:python_build: ('main', 'Oct  3 2024 07:22:19')
2025-02-13 12:58:02,076:INFO:machine: AMD64
2025-02-13 12:58:02,084:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-13 12:58:02,087:INFO:Memory: svmem(total=17008857088, available=6044545024, percent=64.5, used=10964312064, free=6044545024)
2025-02-13 12:58:02,087:INFO:Physical Core: 6
2025-02-13 12:58:02,087:INFO:Logical Core: 12
2025-02-13 12:58:02,087:INFO:Checking libraries
2025-02-13 12:58:02,087:INFO:System:
2025-02-13 12:58:02,087:INFO:    python: 3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]
2025-02-13 12:58:02,087:INFO:executable: C:\Users\syamc\anaconda3\envs\gen_ai\python.exe
2025-02-13 12:58:02,087:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-13 12:58:02,087:INFO:PyCaret required dependencies:
2025-02-13 12:58:02,110:INFO:                 pip: 24.2
2025-02-13 12:58:02,110:INFO:          setuptools: 75.1.0
2025-02-13 12:58:02,110:INFO:             pycaret: 3.3.2
2025-02-13 12:58:02,110:INFO:             IPython: 8.28.0
2025-02-13 12:58:02,110:INFO:          ipywidgets: 8.1.5
2025-02-13 12:58:02,110:INFO:                tqdm: 4.66.5
2025-02-13 12:58:02,110:INFO:               numpy: 1.26.4
2025-02-13 12:58:02,110:INFO:              pandas: 2.1.4
2025-02-13 12:58:02,110:INFO:              jinja2: 3.1.4
2025-02-13 12:58:02,110:INFO:               scipy: 1.11.4
2025-02-13 12:58:02,110:INFO:              joblib: 1.3.2
2025-02-13 12:58:02,110:INFO:             sklearn: 1.4.2
2025-02-13 12:58:02,110:INFO:                pyod: 2.0.3
2025-02-13 12:58:02,110:INFO:            imblearn: 0.13.0
2025-02-13 12:58:02,110:INFO:   category_encoders: 2.7.0
2025-02-13 12:58:02,110:INFO:            lightgbm: 4.5.0
2025-02-13 12:58:02,110:INFO:               numba: 0.61.0
2025-02-13 12:58:02,110:INFO:            requests: 2.32.3
2025-02-13 12:58:02,111:INFO:          matplotlib: 3.7.5
2025-02-13 12:58:02,111:INFO:          scikitplot: 0.3.7
2025-02-13 12:58:02,111:INFO:         yellowbrick: 1.5
2025-02-13 12:58:02,111:INFO:              plotly: 5.24.1
2025-02-13 12:58:02,111:INFO:    plotly-resampler: Not installed
2025-02-13 12:58:02,111:INFO:             kaleido: 0.2.1
2025-02-13 12:58:02,111:INFO:           schemdraw: 0.15
2025-02-13 12:58:02,111:INFO:         statsmodels: 0.14.4
2025-02-13 12:58:02,111:INFO:              sktime: 0.26.0
2025-02-13 12:58:02,111:INFO:               tbats: 1.1.3
2025-02-13 12:58:02,111:INFO:            pmdarima: 2.0.4
2025-02-13 12:58:02,111:INFO:              psutil: 6.0.0
2025-02-13 12:58:02,111:INFO:          markupsafe: 3.0.1
2025-02-13 12:58:02,111:INFO:             pickle5: Not installed
2025-02-13 12:58:02,111:INFO:         cloudpickle: 3.1.1
2025-02-13 12:58:02,111:INFO:         deprecation: 2.1.0
2025-02-13 12:58:02,111:INFO:              xxhash: 3.5.0
2025-02-13 12:58:02,111:INFO:           wurlitzer: Not installed
2025-02-13 12:58:02,112:INFO:PyCaret optional dependencies:
2025-02-13 12:58:02,739:INFO:                shap: Not installed
2025-02-13 12:58:02,739:INFO:           interpret: Not installed
2025-02-13 12:58:02,739:INFO:                umap: Not installed
2025-02-13 12:58:02,739:INFO:     ydata_profiling: Not installed
2025-02-13 12:58:02,739:INFO:  explainerdashboard: Not installed
2025-02-13 12:58:02,739:INFO:             autoviz: Not installed
2025-02-13 12:58:02,739:INFO:           fairlearn: Not installed
2025-02-13 12:58:02,739:INFO:          deepchecks: Not installed
2025-02-13 12:58:02,739:INFO:             xgboost: Not installed
2025-02-13 12:58:02,739:INFO:            catboost: Not installed
2025-02-13 12:58:02,739:INFO:              kmodes: Not installed
2025-02-13 12:58:02,739:INFO:             mlxtend: Not installed
2025-02-13 12:58:02,739:INFO:       statsforecast: Not installed
2025-02-13 12:58:02,739:INFO:        tune_sklearn: Not installed
2025-02-13 12:58:02,739:INFO:                 ray: Not installed
2025-02-13 12:58:02,740:INFO:            hyperopt: Not installed
2025-02-13 12:58:02,740:INFO:              optuna: Not installed
2025-02-13 12:58:02,740:INFO:               skopt: Not installed
2025-02-13 12:58:02,740:INFO:              mlflow: Not installed
2025-02-13 12:58:02,740:INFO:              gradio: Not installed
2025-02-13 12:58:02,740:INFO:             fastapi: 0.115.0
2025-02-13 12:58:02,740:INFO:             uvicorn: 0.31.1
2025-02-13 12:58:02,740:INFO:              m2cgen: Not installed
2025-02-13 12:58:02,740:INFO:           evidently: Not installed
2025-02-13 12:58:02,740:INFO:               fugue: Not installed
2025-02-13 12:58:02,740:INFO:           streamlit: Not installed
2025-02-13 12:58:02,740:INFO:             prophet: Not installed
2025-02-13 12:58:02,740:INFO:None
2025-02-13 12:58:02,740:INFO:Set up data.
2025-02-13 12:58:02,811:INFO:Set up folding strategy.
2025-02-13 12:58:02,812:INFO:Set up train/test split.
2025-02-13 12:58:02,861:INFO:Set up index.
2025-02-13 12:58:02,862:INFO:Assigning column types.
2025-02-13 12:58:02,866:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-13 12:58:02,911:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 12:58:02,915:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:58:02,949:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:02,950:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:02,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 12:58:02,997:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:58:03,025:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,025:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-13 12:58:03,072:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:58:03,099:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,100:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,145:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 12:58:03,174:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,174:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,175:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-13 12:58:03,250:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,250:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,323:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,323:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:03,325:INFO:Preparing preprocessing pipeline...
2025-02-13 12:58:03,327:INFO:Set up simple imputation.
2025-02-13 12:58:03,331:INFO:Set up encoding of categorical features.
2025-02-13 12:58:03,689:INFO:Finished creating preprocessing pipeline.
2025-02-13 12:58:03,697:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\syamc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=['...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Comments'],
                                    transformer=TargetEncoder(cols=['Comments'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-13 12:58:03,697:INFO:Creating final display dataframe.
2025-02-13 12:58:04,233:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (59744, 2)
4        Transformed data shape        (59744, 2)
5   Transformed train set shape        (41820, 2)
6    Transformed test set shape        (17924, 2)
7          Categorical features                 1
8      Rows with missing values              3.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              35ed
2025-02-13 12:58:04,321:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:04,321:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:04,394:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:04,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 12:58:04,395:INFO:setup() successfully completed in 2.32s...............
2025-02-13 12:58:04,396:INFO:Initializing compare_models()
2025-02-13 12:58:04,396:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-13 12:58:04,396:INFO:Checking exceptions
2025-02-13 12:58:04,400:INFO:Preparing display monitor
2025-02-13 12:58:04,403:INFO:Initializing Logistic Regression
2025-02-13 12:58:04,403:INFO:Total runtime is 0.0 minutes
2025-02-13 12:58:04,403:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:04,403:INFO:Initializing create_model()
2025-02-13 12:58:04,404:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:04,404:INFO:Checking exceptions
2025-02-13 12:58:04,404:INFO:Importing libraries
2025-02-13 12:58:04,404:INFO:Copying training dataset
2025-02-13 12:58:04,410:INFO:Defining folds
2025-02-13 12:58:04,410:INFO:Declaring metric variables
2025-02-13 12:58:04,410:INFO:Importing untrained model
2025-02-13 12:58:04,411:INFO:Logistic Regression Imported successfully
2025-02-13 12:58:04,411:INFO:Starting cross validation
2025-02-13 12:58:04,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:10,338:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,351:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,411:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,507:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,704:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,725:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,725:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,771:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,831:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,832:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:10,864:INFO:Calculating mean and std
2025-02-13 12:58:10,865:INFO:Creating metrics dataframe
2025-02-13 12:58:10,867:INFO:Uploading results into container
2025-02-13 12:58:10,868:INFO:Uploading model into container now
2025-02-13 12:58:10,868:INFO:_master_model_container: 1
2025-02-13 12:58:10,868:INFO:_display_container: 2
2025-02-13 12:58:10,869:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-13 12:58:10,869:INFO:create_model() successfully completed......................................
2025-02-13 12:58:10,989:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:10,990:INFO:Creating metrics dataframe
2025-02-13 12:58:10,992:INFO:Initializing K Neighbors Classifier
2025-02-13 12:58:10,992:INFO:Total runtime is 0.10981992483139039 minutes
2025-02-13 12:58:10,992:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:10,993:INFO:Initializing create_model()
2025-02-13 12:58:10,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:10,993:INFO:Checking exceptions
2025-02-13 12:58:10,993:INFO:Importing libraries
2025-02-13 12:58:10,993:INFO:Copying training dataset
2025-02-13 12:58:11,000:INFO:Defining folds
2025-02-13 12:58:11,000:INFO:Declaring metric variables
2025-02-13 12:58:11,000:INFO:Importing untrained model
2025-02-13 12:58:11,000:INFO:K Neighbors Classifier Imported successfully
2025-02-13 12:58:11,001:INFO:Starting cross validation
2025-02-13 12:58:11,002:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:15,077:INFO:Calculating mean and std
2025-02-13 12:58:15,078:INFO:Creating metrics dataframe
2025-02-13 12:58:15,080:INFO:Uploading results into container
2025-02-13 12:58:15,080:INFO:Uploading model into container now
2025-02-13 12:58:15,080:INFO:_master_model_container: 2
2025-02-13 12:58:15,080:INFO:_display_container: 2
2025-02-13 12:58:15,081:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-13 12:58:15,081:INFO:create_model() successfully completed......................................
2025-02-13 12:58:15,193:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:15,193:INFO:Creating metrics dataframe
2025-02-13 12:58:15,196:INFO:Initializing Naive Bayes
2025-02-13 12:58:15,196:INFO:Total runtime is 0.17988188266754152 minutes
2025-02-13 12:58:15,196:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:15,196:INFO:Initializing create_model()
2025-02-13 12:58:15,196:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:15,196:INFO:Checking exceptions
2025-02-13 12:58:15,196:INFO:Importing libraries
2025-02-13 12:58:15,196:INFO:Copying training dataset
2025-02-13 12:58:15,203:INFO:Defining folds
2025-02-13 12:58:15,203:INFO:Declaring metric variables
2025-02-13 12:58:15,203:INFO:Importing untrained model
2025-02-13 12:58:15,204:INFO:Naive Bayes Imported successfully
2025-02-13 12:58:15,204:INFO:Starting cross validation
2025-02-13 12:58:15,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:15,932:INFO:Calculating mean and std
2025-02-13 12:58:15,933:INFO:Creating metrics dataframe
2025-02-13 12:58:15,935:INFO:Uploading results into container
2025-02-13 12:58:15,935:INFO:Uploading model into container now
2025-02-13 12:58:15,935:INFO:_master_model_container: 3
2025-02-13 12:58:15,935:INFO:_display_container: 2
2025-02-13 12:58:15,935:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:58:15,936:INFO:create_model() successfully completed......................................
2025-02-13 12:58:16,039:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:16,039:INFO:Creating metrics dataframe
2025-02-13 12:58:16,042:INFO:Initializing Decision Tree Classifier
2025-02-13 12:58:16,042:INFO:Total runtime is 0.19398076136906942 minutes
2025-02-13 12:58:16,042:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:16,043:INFO:Initializing create_model()
2025-02-13 12:58:16,043:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:16,043:INFO:Checking exceptions
2025-02-13 12:58:16,043:INFO:Importing libraries
2025-02-13 12:58:16,043:INFO:Copying training dataset
2025-02-13 12:58:16,049:INFO:Defining folds
2025-02-13 12:58:16,049:INFO:Declaring metric variables
2025-02-13 12:58:16,049:INFO:Importing untrained model
2025-02-13 12:58:16,049:INFO:Decision Tree Classifier Imported successfully
2025-02-13 12:58:16,049:INFO:Starting cross validation
2025-02-13 12:58:16,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:16,786:INFO:Calculating mean and std
2025-02-13 12:58:16,787:INFO:Creating metrics dataframe
2025-02-13 12:58:16,788:INFO:Uploading results into container
2025-02-13 12:58:16,788:INFO:Uploading model into container now
2025-02-13 12:58:16,790:INFO:_master_model_container: 4
2025-02-13 12:58:16,790:INFO:_display_container: 2
2025-02-13 12:58:16,790:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-02-13 12:58:16,790:INFO:create_model() successfully completed......................................
2025-02-13 12:58:16,894:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:16,894:INFO:Creating metrics dataframe
2025-02-13 12:58:16,896:INFO:Initializing SVM - Linear Kernel
2025-02-13 12:58:16,896:INFO:Total runtime is 0.20821956793467203 minutes
2025-02-13 12:58:16,896:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:16,896:INFO:Initializing create_model()
2025-02-13 12:58:16,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:16,897:INFO:Checking exceptions
2025-02-13 12:58:16,897:INFO:Importing libraries
2025-02-13 12:58:16,897:INFO:Copying training dataset
2025-02-13 12:58:16,903:INFO:Defining folds
2025-02-13 12:58:16,903:INFO:Declaring metric variables
2025-02-13 12:58:16,903:INFO:Importing untrained model
2025-02-13 12:58:16,903:INFO:SVM - Linear Kernel Imported successfully
2025-02-13 12:58:16,903:INFO:Starting cross validation
2025-02-13 12:58:16,905:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:17,468:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,502:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,574:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,591:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,618:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,646:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,650:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,661:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,719:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,722:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,729:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,733:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,753:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,765:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,791:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,799:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,807:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:17,814:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:17,824:INFO:Calculating mean and std
2025-02-13 12:58:17,825:INFO:Creating metrics dataframe
2025-02-13 12:58:17,827:INFO:Uploading results into container
2025-02-13 12:58:17,827:INFO:Uploading model into container now
2025-02-13 12:58:17,827:INFO:_master_model_container: 5
2025-02-13 12:58:17,828:INFO:_display_container: 2
2025-02-13 12:58:17,828:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-13 12:58:17,828:INFO:create_model() successfully completed......................................
2025-02-13 12:58:17,933:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:17,933:INFO:Creating metrics dataframe
2025-02-13 12:58:17,935:INFO:Initializing Ridge Classifier
2025-02-13 12:58:17,935:INFO:Total runtime is 0.22553247610727944 minutes
2025-02-13 12:58:17,936:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:17,936:INFO:Initializing create_model()
2025-02-13 12:58:17,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:17,936:INFO:Checking exceptions
2025-02-13 12:58:17,936:INFO:Importing libraries
2025-02-13 12:58:17,936:INFO:Copying training dataset
2025-02-13 12:58:17,942:INFO:Defining folds
2025-02-13 12:58:17,942:INFO:Declaring metric variables
2025-02-13 12:58:17,943:INFO:Importing untrained model
2025-02-13 12:58:17,943:INFO:Ridge Classifier Imported successfully
2025-02-13 12:58:17,943:INFO:Starting cross validation
2025-02-13 12:58:17,944:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:18,298:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,311:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,385:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,399:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,473:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,484:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,489:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,501:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,567:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,576:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,611:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,619:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,632:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,638:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,645:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,652:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,667:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,672:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,673:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:18,678:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:18,695:INFO:Calculating mean and std
2025-02-13 12:58:18,698:INFO:Creating metrics dataframe
2025-02-13 12:58:18,701:INFO:Uploading results into container
2025-02-13 12:58:18,702:INFO:Uploading model into container now
2025-02-13 12:58:18,703:INFO:_master_model_container: 6
2025-02-13 12:58:18,704:INFO:_display_container: 2
2025-02-13 12:58:18,704:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-02-13 12:58:18,704:INFO:create_model() successfully completed......................................
2025-02-13 12:58:18,822:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:18,822:INFO:Creating metrics dataframe
2025-02-13 12:58:18,824:INFO:Initializing Random Forest Classifier
2025-02-13 12:58:18,825:INFO:Total runtime is 0.24035780827204384 minutes
2025-02-13 12:58:18,825:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:18,825:INFO:Initializing create_model()
2025-02-13 12:58:18,825:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:18,825:INFO:Checking exceptions
2025-02-13 12:58:18,825:INFO:Importing libraries
2025-02-13 12:58:18,825:INFO:Copying training dataset
2025-02-13 12:58:18,831:INFO:Defining folds
2025-02-13 12:58:18,831:INFO:Declaring metric variables
2025-02-13 12:58:18,831:INFO:Importing untrained model
2025-02-13 12:58:18,831:INFO:Random Forest Classifier Imported successfully
2025-02-13 12:58:18,831:INFO:Starting cross validation
2025-02-13 12:58:18,832:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:21,236:INFO:Calculating mean and std
2025-02-13 12:58:21,237:INFO:Creating metrics dataframe
2025-02-13 12:58:21,239:INFO:Uploading results into container
2025-02-13 12:58:21,239:INFO:Uploading model into container now
2025-02-13 12:58:21,240:INFO:_master_model_container: 7
2025-02-13 12:58:21,240:INFO:_display_container: 2
2025-02-13 12:58:21,240:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-02-13 12:58:21,241:INFO:create_model() successfully completed......................................
2025-02-13 12:58:21,353:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:21,353:INFO:Creating metrics dataframe
2025-02-13 12:58:21,355:INFO:Initializing Quadratic Discriminant Analysis
2025-02-13 12:58:21,355:INFO:Total runtime is 0.2825376788775126 minutes
2025-02-13 12:58:21,355:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:21,355:INFO:Initializing create_model()
2025-02-13 12:58:21,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:21,356:INFO:Checking exceptions
2025-02-13 12:58:21,356:INFO:Importing libraries
2025-02-13 12:58:21,356:INFO:Copying training dataset
2025-02-13 12:58:21,362:INFO:Defining folds
2025-02-13 12:58:21,362:INFO:Declaring metric variables
2025-02-13 12:58:21,362:INFO:Importing untrained model
2025-02-13 12:58:21,362:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-13 12:58:21,363:INFO:Starting cross validation
2025-02-13 12:58:21,364:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:21,709:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:21,789:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:21,861:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:21,939:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:21,992:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:22,037:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:22,080:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:22,105:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:22,106:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:22,120:INFO:Calculating mean and std
2025-02-13 12:58:22,121:INFO:Creating metrics dataframe
2025-02-13 12:58:22,123:INFO:Uploading results into container
2025-02-13 12:58:22,123:INFO:Uploading model into container now
2025-02-13 12:58:22,123:INFO:_master_model_container: 8
2025-02-13 12:58:22,123:INFO:_display_container: 2
2025-02-13 12:58:22,124:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-13 12:58:22,124:INFO:create_model() successfully completed......................................
2025-02-13 12:58:22,225:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:22,225:INFO:Creating metrics dataframe
2025-02-13 12:58:22,227:INFO:Initializing Ada Boost Classifier
2025-02-13 12:58:22,227:INFO:Total runtime is 0.2970660090446472 minutes
2025-02-13 12:58:22,227:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:22,227:INFO:Initializing create_model()
2025-02-13 12:58:22,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:22,228:INFO:Checking exceptions
2025-02-13 12:58:22,228:INFO:Importing libraries
2025-02-13 12:58:22,228:INFO:Copying training dataset
2025-02-13 12:58:22,234:INFO:Defining folds
2025-02-13 12:58:22,234:INFO:Declaring metric variables
2025-02-13 12:58:22,234:INFO:Importing untrained model
2025-02-13 12:58:22,234:INFO:Ada Boost Classifier Imported successfully
2025-02-13 12:58:22,234:INFO:Starting cross validation
2025-02-13 12:58:22,235:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:22,572:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,663:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,680:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,778:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,860:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,966:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:22,991:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:23,044:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:23,109:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:23,164:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 12:58:24,035:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,117:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,166:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,205:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,282:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,305:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,356:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,388:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,405:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,454:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:24,485:INFO:Calculating mean and std
2025-02-13 12:58:24,488:INFO:Creating metrics dataframe
2025-02-13 12:58:24,494:INFO:Uploading results into container
2025-02-13 12:58:24,495:INFO:Uploading model into container now
2025-02-13 12:58:24,497:INFO:_master_model_container: 9
2025-02-13 12:58:24,497:INFO:_display_container: 2
2025-02-13 12:58:24,498:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-02-13 12:58:24,498:INFO:create_model() successfully completed......................................
2025-02-13 12:58:24,620:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:24,621:INFO:Creating metrics dataframe
2025-02-13 12:58:24,623:INFO:Initializing Gradient Boosting Classifier
2025-02-13 12:58:24,623:INFO:Total runtime is 0.33699920177459713 minutes
2025-02-13 12:58:24,623:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:24,624:INFO:Initializing create_model()
2025-02-13 12:58:24,624:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:24,624:INFO:Checking exceptions
2025-02-13 12:58:24,624:INFO:Importing libraries
2025-02-13 12:58:24,624:INFO:Copying training dataset
2025-02-13 12:58:24,631:INFO:Defining folds
2025-02-13 12:58:24,631:INFO:Declaring metric variables
2025-02-13 12:58:24,631:INFO:Importing untrained model
2025-02-13 12:58:24,631:INFO:Gradient Boosting Classifier Imported successfully
2025-02-13 12:58:24,631:INFO:Starting cross validation
2025-02-13 12:58:24,632:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:33,242:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,247:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,284:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,387:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,401:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,412:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,473:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,502:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,528:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,585:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:33,612:INFO:Calculating mean and std
2025-02-13 12:58:33,613:INFO:Creating metrics dataframe
2025-02-13 12:58:33,615:INFO:Uploading results into container
2025-02-13 12:58:33,616:INFO:Uploading model into container now
2025-02-13 12:58:33,616:INFO:_master_model_container: 10
2025-02-13 12:58:33,616:INFO:_display_container: 2
2025-02-13 12:58:33,617:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-13 12:58:33,617:INFO:create_model() successfully completed......................................
2025-02-13 12:58:33,737:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:33,737:INFO:Creating metrics dataframe
2025-02-13 12:58:33,740:INFO:Initializing Linear Discriminant Analysis
2025-02-13 12:58:33,740:INFO:Total runtime is 0.48894618749618524 minutes
2025-02-13 12:58:33,740:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:33,741:INFO:Initializing create_model()
2025-02-13 12:58:33,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:33,741:INFO:Checking exceptions
2025-02-13 12:58:33,741:INFO:Importing libraries
2025-02-13 12:58:33,741:INFO:Copying training dataset
2025-02-13 12:58:33,748:INFO:Defining folds
2025-02-13 12:58:33,748:INFO:Declaring metric variables
2025-02-13 12:58:33,749:INFO:Importing untrained model
2025-02-13 12:58:33,749:INFO:Linear Discriminant Analysis Imported successfully
2025-02-13 12:58:33,749:INFO:Starting cross validation
2025-02-13 12:58:33,750:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:34,122:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,175:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,230:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,291:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,325:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,382:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,411:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,463:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,474:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,482:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 12:58:34,513:INFO:Calculating mean and std
2025-02-13 12:58:34,516:INFO:Creating metrics dataframe
2025-02-13 12:58:34,522:INFO:Uploading results into container
2025-02-13 12:58:34,523:INFO:Uploading model into container now
2025-02-13 12:58:34,524:INFO:_master_model_container: 11
2025-02-13 12:58:34,524:INFO:_display_container: 2
2025-02-13 12:58:34,525:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-13 12:58:34,526:INFO:create_model() successfully completed......................................
2025-02-13 12:58:34,657:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:34,657:INFO:Creating metrics dataframe
2025-02-13 12:58:34,660:INFO:Initializing Extra Trees Classifier
2025-02-13 12:58:34,660:INFO:Total runtime is 0.5042844692866006 minutes
2025-02-13 12:58:34,660:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:34,660:INFO:Initializing create_model()
2025-02-13 12:58:34,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:34,660:INFO:Checking exceptions
2025-02-13 12:58:34,660:INFO:Importing libraries
2025-02-13 12:58:34,660:INFO:Copying training dataset
2025-02-13 12:58:34,667:INFO:Defining folds
2025-02-13 12:58:34,667:INFO:Declaring metric variables
2025-02-13 12:58:34,667:INFO:Importing untrained model
2025-02-13 12:58:34,668:INFO:Extra Trees Classifier Imported successfully
2025-02-13 12:58:34,668:INFO:Starting cross validation
2025-02-13 12:58:34,669:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:36,905:INFO:Calculating mean and std
2025-02-13 12:58:36,908:INFO:Creating metrics dataframe
2025-02-13 12:58:36,914:INFO:Uploading results into container
2025-02-13 12:58:36,915:INFO:Uploading model into container now
2025-02-13 12:58:36,917:INFO:_master_model_container: 12
2025-02-13 12:58:36,917:INFO:_display_container: 2
2025-02-13 12:58:36,918:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-02-13 12:58:36,918:INFO:create_model() successfully completed......................................
2025-02-13 12:58:37,051:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:37,053:INFO:Creating metrics dataframe
2025-02-13 12:58:37,055:INFO:Initializing Light Gradient Boosting Machine
2025-02-13 12:58:37,055:INFO:Total runtime is 0.5441906611124674 minutes
2025-02-13 12:58:37,055:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:37,056:INFO:Initializing create_model()
2025-02-13 12:58:37,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:37,056:INFO:Checking exceptions
2025-02-13 12:58:37,056:INFO:Importing libraries
2025-02-13 12:58:37,056:INFO:Copying training dataset
2025-02-13 12:58:37,063:INFO:Defining folds
2025-02-13 12:58:37,063:INFO:Declaring metric variables
2025-02-13 12:58:37,063:INFO:Importing untrained model
2025-02-13 12:58:37,064:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 12:58:37,064:INFO:Starting cross validation
2025-02-13 12:58:37,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:43,600:INFO:Calculating mean and std
2025-02-13 12:58:43,601:INFO:Creating metrics dataframe
2025-02-13 12:58:43,605:INFO:Uploading results into container
2025-02-13 12:58:43,606:INFO:Uploading model into container now
2025-02-13 12:58:43,606:INFO:_master_model_container: 13
2025-02-13 12:58:43,606:INFO:_display_container: 2
2025-02-13 12:58:43,607:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 12:58:43,607:INFO:create_model() successfully completed......................................
2025-02-13 12:58:43,740:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:43,740:INFO:Creating metrics dataframe
2025-02-13 12:58:43,743:INFO:Initializing Dummy Classifier
2025-02-13 12:58:43,743:INFO:Total runtime is 0.655663780371348 minutes
2025-02-13 12:58:43,744:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:43,744:INFO:Initializing create_model()
2025-02-13 12:58:43,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367D1A1C30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:43,744:INFO:Checking exceptions
2025-02-13 12:58:43,744:INFO:Importing libraries
2025-02-13 12:58:43,744:INFO:Copying training dataset
2025-02-13 12:58:43,753:INFO:Defining folds
2025-02-13 12:58:43,753:INFO:Declaring metric variables
2025-02-13 12:58:43,753:INFO:Importing untrained model
2025-02-13 12:58:43,754:INFO:Dummy Classifier Imported successfully
2025-02-13 12:58:43,754:INFO:Starting cross validation
2025-02-13 12:58:43,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:44,191:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,319:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,404:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,454:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,517:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,554:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,605:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,623:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,632:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,653:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 12:58:44,675:INFO:Calculating mean and std
2025-02-13 12:58:44,678:INFO:Creating metrics dataframe
2025-02-13 12:58:44,683:INFO:Uploading results into container
2025-02-13 12:58:44,685:INFO:Uploading model into container now
2025-02-13 12:58:44,686:INFO:_master_model_container: 14
2025-02-13 12:58:44,686:INFO:_display_container: 2
2025-02-13 12:58:44,687:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-02-13 12:58:44,687:INFO:create_model() successfully completed......................................
2025-02-13 12:58:44,818:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:44,818:INFO:Creating metrics dataframe
2025-02-13 12:58:44,822:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-13 12:58:44,823:INFO:Initializing create_model()
2025-02-13 12:58:44,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:44,823:INFO:Checking exceptions
2025-02-13 12:58:44,824:INFO:Importing libraries
2025-02-13 12:58:44,824:INFO:Copying training dataset
2025-02-13 12:58:44,830:INFO:Defining folds
2025-02-13 12:58:44,830:INFO:Declaring metric variables
2025-02-13 12:58:44,830:INFO:Importing untrained model
2025-02-13 12:58:44,830:INFO:Declaring custom model
2025-02-13 12:58:44,831:INFO:Naive Bayes Imported successfully
2025-02-13 12:58:44,832:INFO:Cross validation set to False
2025-02-13 12:58:44,832:INFO:Fitting Model
2025-02-13 12:58:44,940:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:58:44,940:INFO:create_model() successfully completed......................................
2025-02-13 12:58:45,051:INFO:_master_model_container: 14
2025-02-13 12:58:45,051:INFO:_display_container: 2
2025-02-13 12:58:45,051:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:58:45,051:INFO:compare_models() successfully completed......................................
2025-02-13 12:58:45,051:INFO:Initializing tune_model()
2025-02-13 12:58:45,051:INFO:tune_model(estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>)
2025-02-13 12:58:45,051:INFO:Checking exceptions
2025-02-13 12:58:45,055:INFO:Copying training dataset
2025-02-13 12:58:45,061:INFO:Checking base model
2025-02-13 12:58:45,061:INFO:Base model : Naive Bayes
2025-02-13 12:58:45,061:INFO:Declaring metric variables
2025-02-13 12:58:45,061:INFO:Defining Hyperparameters
2025-02-13 12:58:45,176:INFO:Tuning with n_jobs=-1
2025-02-13 12:58:45,176:INFO:Initializing RandomizedSearchCV
2025-02-13 12:58:52,170:INFO:best_params: {'actual_estimator__var_smoothing': 2e-07}
2025-02-13 12:58:52,172:INFO:Hyperparameter search completed
2025-02-13 12:58:52,172:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:52,173:INFO:Initializing create_model()
2025-02-13 12:58:52,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002367AE6BA30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'var_smoothing': 2e-07})
2025-02-13 12:58:52,174:INFO:Checking exceptions
2025-02-13 12:58:52,174:INFO:Importing libraries
2025-02-13 12:58:52,174:INFO:Copying training dataset
2025-02-13 12:58:52,201:INFO:Defining folds
2025-02-13 12:58:52,201:INFO:Declaring metric variables
2025-02-13 12:58:52,202:INFO:Importing untrained model
2025-02-13 12:58:52,202:INFO:Declaring custom model
2025-02-13 12:58:52,204:INFO:Naive Bayes Imported successfully
2025-02-13 12:58:52,205:INFO:Starting cross validation
2025-02-13 12:58:52,208:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:52,977:INFO:Calculating mean and std
2025-02-13 12:58:52,980:INFO:Creating metrics dataframe
2025-02-13 12:58:52,984:INFO:Finalizing model
2025-02-13 12:58:53,097:INFO:Uploading results into container
2025-02-13 12:58:53,098:INFO:Uploading model into container now
2025-02-13 12:58:53,098:INFO:_master_model_container: 15
2025-02-13 12:58:53,098:INFO:_display_container: 3
2025-02-13 12:58:53,098:INFO:GaussianNB(priors=None, var_smoothing=2e-07)
2025-02-13 12:58:53,098:INFO:create_model() successfully completed......................................
2025-02-13 12:58:53,203:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:53,203:INFO:choose_better activated
2025-02-13 12:58:53,204:INFO:SubProcess create_model() called ==================================
2025-02-13 12:58:53,204:INFO:Initializing create_model()
2025-02-13 12:58:53,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023679283730>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 12:58:53,204:INFO:Checking exceptions
2025-02-13 12:58:53,205:INFO:Importing libraries
2025-02-13 12:58:53,205:INFO:Copying training dataset
2025-02-13 12:58:53,212:INFO:Defining folds
2025-02-13 12:58:53,212:INFO:Declaring metric variables
2025-02-13 12:58:53,212:INFO:Importing untrained model
2025-02-13 12:58:53,212:INFO:Declaring custom model
2025-02-13 12:58:53,212:INFO:Naive Bayes Imported successfully
2025-02-13 12:58:53,212:INFO:Starting cross validation
2025-02-13 12:58:53,213:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 12:58:54,009:INFO:Calculating mean and std
2025-02-13 12:58:54,010:INFO:Creating metrics dataframe
2025-02-13 12:58:54,012:INFO:Finalizing model
2025-02-13 12:58:54,120:INFO:Uploading results into container
2025-02-13 12:58:54,121:INFO:Uploading model into container now
2025-02-13 12:58:54,121:INFO:_master_model_container: 16
2025-02-13 12:58:54,121:INFO:_display_container: 4
2025-02-13 12:58:54,121:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:58:54,121:INFO:create_model() successfully completed......................................
2025-02-13 12:58:54,220:INFO:SubProcess create_model() end ==================================
2025-02-13 12:58:54,220:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for Accuracy is 0.3813
2025-02-13 12:58:54,220:INFO:GaussianNB(priors=None, var_smoothing=2e-07) result for Accuracy is 0.3813
2025-02-13 12:58:54,220:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2025-02-13 12:58:54,220:INFO:choose_better completed
2025-02-13 12:58:54,221:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-02-13 12:58:54,231:INFO:_master_model_container: 16
2025-02-13 12:58:54,231:INFO:_display_container: 3
2025-02-13 12:58:54,231:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 12:58:54,231:INFO:tune_model() successfully completed......................................
2025-02-13 15:20:03,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:20:03,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:20:03,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:20:03,734:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:20:52,510:INFO:PyCaret ClassificationExperiment
2025-02-13 15:20:52,511:INFO:Logging name: clf-default-name
2025-02-13 15:20:52,511:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-13 15:20:52,511:INFO:version 3.3.2
2025-02-13 15:20:52,511:INFO:Initializing setup()
2025-02-13 15:20:52,512:INFO:self.USI: c58e
2025-02-13 15:20:52,512:INFO:self._variable_keys: {'_ml_usecase', 'data', 'X_train', 'exp_name_log', 'html_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'log_plots_param', 'fold_groups_param', 'USI', 'target_param', 'logging_param', 'idx', 'gpu_n_jobs_param', 'X', 'is_multiclass', 'y_train', 'seed', 'gpu_param', 'n_jobs_param', 'memory', 'pipeline', '_available_plots', 'y', 'fix_imbalance', 'exp_id', 'y_test'}
2025-02-13 15:20:52,512:INFO:Checking environment
2025-02-13 15:20:52,512:INFO:python_version: 3.10.15
2025-02-13 15:20:52,513:INFO:python_build: ('main', 'Oct  3 2024 07:22:19')
2025-02-13 15:20:52,513:INFO:machine: AMD64
2025-02-13 15:20:52,548:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-13 15:20:52,556:INFO:Memory: svmem(total=17008857088, available=7000457216, percent=58.8, used=10008399872, free=7000457216)
2025-02-13 15:20:52,556:INFO:Physical Core: 6
2025-02-13 15:20:52,558:INFO:Logical Core: 12
2025-02-13 15:20:52,558:INFO:Checking libraries
2025-02-13 15:20:52,558:INFO:System:
2025-02-13 15:20:52,558:INFO:    python: 3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]
2025-02-13 15:20:52,559:INFO:executable: C:\Users\syamc\anaconda3\envs\gen_ai\python.exe
2025-02-13 15:20:52,559:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-13 15:20:52,559:INFO:PyCaret required dependencies:
2025-02-13 15:20:52,678:INFO:                 pip: 24.2
2025-02-13 15:20:52,678:INFO:          setuptools: 75.1.0
2025-02-13 15:20:52,678:INFO:             pycaret: 3.3.2
2025-02-13 15:20:52,678:INFO:             IPython: 8.28.0
2025-02-13 15:20:52,678:INFO:          ipywidgets: 8.1.5
2025-02-13 15:20:52,678:INFO:                tqdm: 4.66.5
2025-02-13 15:20:52,680:INFO:               numpy: 1.26.4
2025-02-13 15:20:52,680:INFO:              pandas: 2.1.4
2025-02-13 15:20:52,680:INFO:              jinja2: 3.1.4
2025-02-13 15:20:52,680:INFO:               scipy: 1.11.4
2025-02-13 15:20:52,680:INFO:              joblib: 1.3.2
2025-02-13 15:20:52,681:INFO:             sklearn: 1.4.2
2025-02-13 15:20:52,681:INFO:                pyod: 2.0.3
2025-02-13 15:20:52,681:INFO:            imblearn: 0.13.0
2025-02-13 15:20:52,681:INFO:   category_encoders: 2.7.0
2025-02-13 15:20:52,681:INFO:            lightgbm: 4.5.0
2025-02-13 15:20:52,681:INFO:               numba: 0.61.0
2025-02-13 15:20:52,681:INFO:            requests: 2.32.3
2025-02-13 15:20:52,681:INFO:          matplotlib: 3.7.5
2025-02-13 15:20:52,682:INFO:          scikitplot: 0.3.7
2025-02-13 15:20:52,682:INFO:         yellowbrick: 1.5
2025-02-13 15:20:52,682:INFO:              plotly: 5.24.1
2025-02-13 15:20:52,682:INFO:    plotly-resampler: Not installed
2025-02-13 15:20:52,682:INFO:             kaleido: 0.2.1
2025-02-13 15:20:52,682:INFO:           schemdraw: 0.15
2025-02-13 15:20:52,682:INFO:         statsmodels: 0.14.4
2025-02-13 15:20:52,682:INFO:              sktime: 0.26.0
2025-02-13 15:20:52,682:INFO:               tbats: 1.1.3
2025-02-13 15:20:52,683:INFO:            pmdarima: 2.0.4
2025-02-13 15:20:52,683:INFO:              psutil: 6.0.0
2025-02-13 15:20:52,683:INFO:          markupsafe: 3.0.1
2025-02-13 15:20:52,683:INFO:             pickle5: Not installed
2025-02-13 15:20:52,683:INFO:         cloudpickle: 3.1.1
2025-02-13 15:20:52,683:INFO:         deprecation: 2.1.0
2025-02-13 15:20:52,683:INFO:              xxhash: 3.5.0
2025-02-13 15:20:52,684:INFO:           wurlitzer: Not installed
2025-02-13 15:20:52,684:INFO:PyCaret optional dependencies:
2025-02-13 15:20:54,182:INFO:                shap: Not installed
2025-02-13 15:20:54,183:INFO:           interpret: Not installed
2025-02-13 15:20:54,183:INFO:                umap: Not installed
2025-02-13 15:20:54,183:INFO:     ydata_profiling: Not installed
2025-02-13 15:20:54,183:INFO:  explainerdashboard: Not installed
2025-02-13 15:20:54,183:INFO:             autoviz: Not installed
2025-02-13 15:20:54,183:INFO:           fairlearn: Not installed
2025-02-13 15:20:54,183:INFO:          deepchecks: Not installed
2025-02-13 15:20:54,183:INFO:             xgboost: Not installed
2025-02-13 15:20:54,183:INFO:            catboost: Not installed
2025-02-13 15:20:54,184:INFO:              kmodes: Not installed
2025-02-13 15:20:54,184:INFO:             mlxtend: Not installed
2025-02-13 15:20:54,184:INFO:       statsforecast: Not installed
2025-02-13 15:20:54,184:INFO:        tune_sklearn: Not installed
2025-02-13 15:20:54,184:INFO:                 ray: Not installed
2025-02-13 15:20:54,184:INFO:            hyperopt: Not installed
2025-02-13 15:20:54,184:INFO:              optuna: Not installed
2025-02-13 15:20:54,184:INFO:               skopt: Not installed
2025-02-13 15:20:54,184:INFO:              mlflow: Not installed
2025-02-13 15:20:54,184:INFO:              gradio: Not installed
2025-02-13 15:20:54,184:INFO:             fastapi: 0.115.0
2025-02-13 15:20:54,185:INFO:             uvicorn: 0.31.1
2025-02-13 15:20:54,185:INFO:              m2cgen: Not installed
2025-02-13 15:20:54,185:INFO:           evidently: Not installed
2025-02-13 15:20:54,185:INFO:               fugue: Not installed
2025-02-13 15:20:54,185:INFO:           streamlit: Not installed
2025-02-13 15:20:54,185:INFO:             prophet: Not installed
2025-02-13 15:20:54,185:INFO:None
2025-02-13 15:20:54,185:INFO:Set up data.
2025-02-13 15:20:54,560:INFO:Set up folding strategy.
2025-02-13 15:20:54,560:INFO:Set up train/test split.
2025-02-13 15:20:54,642:INFO:Set up index.
2025-02-13 15:20:54,645:INFO:Assigning column types.
2025-02-13 15:20:54,650:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-13 15:20:54,733:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 15:20:54,739:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:20:54,811:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:54,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:54,925:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 15:20:54,927:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:20:55,009:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,010:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,011:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-13 15:20:55,152:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:20:55,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,215:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,308:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:20:55,365:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,366:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,367:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-13 15:20:55,514:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,515:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,675:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,676:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:55,716:INFO:Preparing preprocessing pipeline...
2025-02-13 15:20:55,719:INFO:Set up simple imputation.
2025-02-13 15:20:55,727:INFO:Set up encoding of categorical features.
2025-02-13 15:20:56,557:INFO:Finished creating preprocessing pipeline.
2025-02-13 15:20:56,570:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\syamc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=['...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Comments'],
                                    transformer=TargetEncoder(cols=['Comments'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-13 15:20:56,570:INFO:Creating final display dataframe.
2025-02-13 15:20:58,477:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (59196, 2)
4        Transformed data shape        (59196, 2)
5   Transformed train set shape        (41437, 2)
6    Transformed test set shape        (17759, 2)
7          Categorical features                 1
8      Rows with missing values              2.2%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c58e
2025-02-13 15:20:58,713:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:58,714:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:58,868:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:58,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:20:58,871:INFO:setup() successfully completed in 6.37s...............
2025-02-13 15:20:58,871:INFO:Initializing compare_models()
2025-02-13 15:20:58,871:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-13 15:20:58,871:INFO:Checking exceptions
2025-02-13 15:20:58,882:INFO:Preparing display monitor
2025-02-13 15:20:58,889:INFO:Initializing Logistic Regression
2025-02-13 15:20:58,890:INFO:Total runtime is 1.654624938964844e-05 minutes
2025-02-13 15:20:58,890:INFO:SubProcess create_model() called ==================================
2025-02-13 15:20:58,891:INFO:Initializing create_model()
2025-02-13 15:20:58,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:20:58,891:INFO:Checking exceptions
2025-02-13 15:20:58,891:INFO:Importing libraries
2025-02-13 15:20:58,891:INFO:Copying training dataset
2025-02-13 15:20:58,912:INFO:Defining folds
2025-02-13 15:20:58,912:INFO:Declaring metric variables
2025-02-13 15:20:58,913:INFO:Importing untrained model
2025-02-13 15:20:58,915:INFO:Logistic Regression Imported successfully
2025-02-13 15:20:58,916:INFO:Starting cross validation
2025-02-13 15:20:58,920:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:07,393:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,394:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,394:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,395:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,408:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,472:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,498:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,498:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,505:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,565:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:07,619:INFO:Calculating mean and std
2025-02-13 15:21:07,621:INFO:Creating metrics dataframe
2025-02-13 15:21:07,626:INFO:Uploading results into container
2025-02-13 15:21:07,627:INFO:Uploading model into container now
2025-02-13 15:21:07,628:INFO:_master_model_container: 1
2025-02-13 15:21:07,628:INFO:_display_container: 2
2025-02-13 15:21:07,629:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-13 15:21:07,629:INFO:create_model() successfully completed......................................
2025-02-13 15:21:07,770:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:07,770:INFO:Creating metrics dataframe
2025-02-13 15:21:07,773:INFO:Initializing K Neighbors Classifier
2025-02-13 15:21:07,773:INFO:Total runtime is 0.14805211226145426 minutes
2025-02-13 15:21:07,773:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:07,773:INFO:Initializing create_model()
2025-02-13 15:21:07,774:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:07,774:INFO:Checking exceptions
2025-02-13 15:21:07,774:INFO:Importing libraries
2025-02-13 15:21:07,774:INFO:Copying training dataset
2025-02-13 15:21:07,783:INFO:Defining folds
2025-02-13 15:21:07,783:INFO:Declaring metric variables
2025-02-13 15:21:07,783:INFO:Importing untrained model
2025-02-13 15:21:07,784:INFO:K Neighbors Classifier Imported successfully
2025-02-13 15:21:07,784:INFO:Starting cross validation
2025-02-13 15:21:07,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:15,904:INFO:Calculating mean and std
2025-02-13 15:21:15,905:INFO:Creating metrics dataframe
2025-02-13 15:21:15,908:INFO:Uploading results into container
2025-02-13 15:21:15,908:INFO:Uploading model into container now
2025-02-13 15:21:15,909:INFO:_master_model_container: 2
2025-02-13 15:21:15,909:INFO:_display_container: 2
2025-02-13 15:21:15,909:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-13 15:21:15,909:INFO:create_model() successfully completed......................................
2025-02-13 15:21:16,042:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:16,042:INFO:Creating metrics dataframe
2025-02-13 15:21:16,045:INFO:Initializing Naive Bayes
2025-02-13 15:21:16,045:INFO:Total runtime is 0.2859330574671427 minutes
2025-02-13 15:21:16,045:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:16,045:INFO:Initializing create_model()
2025-02-13 15:21:16,045:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:16,046:INFO:Checking exceptions
2025-02-13 15:21:16,046:INFO:Importing libraries
2025-02-13 15:21:16,046:INFO:Copying training dataset
2025-02-13 15:21:16,055:INFO:Defining folds
2025-02-13 15:21:16,055:INFO:Declaring metric variables
2025-02-13 15:21:16,056:INFO:Importing untrained model
2025-02-13 15:21:16,056:INFO:Naive Bayes Imported successfully
2025-02-13 15:21:16,056:INFO:Starting cross validation
2025-02-13 15:21:16,057:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:16,915:INFO:Calculating mean and std
2025-02-13 15:21:16,918:INFO:Creating metrics dataframe
2025-02-13 15:21:16,926:INFO:Uploading results into container
2025-02-13 15:21:16,928:INFO:Uploading model into container now
2025-02-13 15:21:16,929:INFO:_master_model_container: 3
2025-02-13 15:21:16,930:INFO:_display_container: 2
2025-02-13 15:21:16,931:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-13 15:21:16,931:INFO:create_model() successfully completed......................................
2025-02-13 15:21:17,110:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:17,111:INFO:Creating metrics dataframe
2025-02-13 15:21:17,114:INFO:Initializing Decision Tree Classifier
2025-02-13 15:21:17,114:INFO:Total runtime is 0.3037362853686014 minutes
2025-02-13 15:21:17,114:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:17,114:INFO:Initializing create_model()
2025-02-13 15:21:17,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:17,114:INFO:Checking exceptions
2025-02-13 15:21:17,114:INFO:Importing libraries
2025-02-13 15:21:17,115:INFO:Copying training dataset
2025-02-13 15:21:17,123:INFO:Defining folds
2025-02-13 15:21:17,123:INFO:Declaring metric variables
2025-02-13 15:21:17,123:INFO:Importing untrained model
2025-02-13 15:21:17,123:INFO:Decision Tree Classifier Imported successfully
2025-02-13 15:21:17,124:INFO:Starting cross validation
2025-02-13 15:21:17,125:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:17,989:INFO:Calculating mean and std
2025-02-13 15:21:17,990:INFO:Creating metrics dataframe
2025-02-13 15:21:17,993:INFO:Uploading results into container
2025-02-13 15:21:17,994:INFO:Uploading model into container now
2025-02-13 15:21:17,994:INFO:_master_model_container: 4
2025-02-13 15:21:17,994:INFO:_display_container: 2
2025-02-13 15:21:17,994:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-02-13 15:21:17,994:INFO:create_model() successfully completed......................................
2025-02-13 15:21:18,178:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:18,178:INFO:Creating metrics dataframe
2025-02-13 15:21:18,181:INFO:Initializing SVM - Linear Kernel
2025-02-13 15:21:18,181:INFO:Total runtime is 0.3215242465337117 minutes
2025-02-13 15:21:18,181:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:18,181:INFO:Initializing create_model()
2025-02-13 15:21:18,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:18,181:INFO:Checking exceptions
2025-02-13 15:21:18,181:INFO:Importing libraries
2025-02-13 15:21:18,183:INFO:Copying training dataset
2025-02-13 15:21:18,189:INFO:Defining folds
2025-02-13 15:21:18,189:INFO:Declaring metric variables
2025-02-13 15:21:18,191:INFO:Importing untrained model
2025-02-13 15:21:18,191:INFO:SVM - Linear Kernel Imported successfully
2025-02-13 15:21:18,191:INFO:Starting cross validation
2025-02-13 15:21:18,192:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:19,021:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,041:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,103:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,140:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,159:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,235:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,262:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,311:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,329:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,343:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,347:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,356:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,381:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,410:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:19,420:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:19,439:INFO:Calculating mean and std
2025-02-13 15:21:19,440:INFO:Creating metrics dataframe
2025-02-13 15:21:19,443:INFO:Uploading results into container
2025-02-13 15:21:19,443:INFO:Uploading model into container now
2025-02-13 15:21:19,444:INFO:_master_model_container: 5
2025-02-13 15:21:19,444:INFO:_display_container: 2
2025-02-13 15:21:19,445:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-13 15:21:19,445:INFO:create_model() successfully completed......................................
2025-02-13 15:21:19,576:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:19,576:INFO:Creating metrics dataframe
2025-02-13 15:21:19,580:INFO:Initializing Ridge Classifier
2025-02-13 15:21:19,580:INFO:Total runtime is 0.3448494871457417 minutes
2025-02-13 15:21:19,580:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:19,580:INFO:Initializing create_model()
2025-02-13 15:21:19,580:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:19,581:INFO:Checking exceptions
2025-02-13 15:21:19,581:INFO:Importing libraries
2025-02-13 15:21:19,581:INFO:Copying training dataset
2025-02-13 15:21:19,589:INFO:Defining folds
2025-02-13 15:21:19,589:INFO:Declaring metric variables
2025-02-13 15:21:19,589:INFO:Importing untrained model
2025-02-13 15:21:19,590:INFO:Ridge Classifier Imported successfully
2025-02-13 15:21:19,590:INFO:Starting cross validation
2025-02-13 15:21:19,591:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:20,065:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,086:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,159:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,179:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,216:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,233:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,264:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,285:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,360:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,377:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,412:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,437:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,439:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,464:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,474:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,477:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,488:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,504:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:20,511:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:20,528:INFO:Calculating mean and std
2025-02-13 15:21:20,530:INFO:Creating metrics dataframe
2025-02-13 15:21:20,533:INFO:Uploading results into container
2025-02-13 15:21:20,533:INFO:Uploading model into container now
2025-02-13 15:21:20,534:INFO:_master_model_container: 6
2025-02-13 15:21:20,534:INFO:_display_container: 2
2025-02-13 15:21:20,534:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-02-13 15:21:20,534:INFO:create_model() successfully completed......................................
2025-02-13 15:21:20,683:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:20,683:INFO:Creating metrics dataframe
2025-02-13 15:21:20,686:INFO:Initializing Random Forest Classifier
2025-02-13 15:21:20,686:INFO:Total runtime is 0.3632673144340514 minutes
2025-02-13 15:21:20,687:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:20,687:INFO:Initializing create_model()
2025-02-13 15:21:20,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:20,687:INFO:Checking exceptions
2025-02-13 15:21:20,687:INFO:Importing libraries
2025-02-13 15:21:20,687:INFO:Copying training dataset
2025-02-13 15:21:20,696:INFO:Defining folds
2025-02-13 15:21:20,696:INFO:Declaring metric variables
2025-02-13 15:21:20,696:INFO:Importing untrained model
2025-02-13 15:21:20,696:INFO:Random Forest Classifier Imported successfully
2025-02-13 15:21:20,698:INFO:Starting cross validation
2025-02-13 15:21:20,699:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:23,618:INFO:Calculating mean and std
2025-02-13 15:21:23,620:INFO:Creating metrics dataframe
2025-02-13 15:21:23,622:INFO:Uploading results into container
2025-02-13 15:21:23,622:INFO:Uploading model into container now
2025-02-13 15:21:23,622:INFO:_master_model_container: 7
2025-02-13 15:21:23,624:INFO:_display_container: 2
2025-02-13 15:21:23,624:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-02-13 15:21:23,624:INFO:create_model() successfully completed......................................
2025-02-13 15:21:23,767:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:23,767:INFO:Creating metrics dataframe
2025-02-13 15:21:23,770:INFO:Initializing Quadratic Discriminant Analysis
2025-02-13 15:21:23,770:INFO:Total runtime is 0.41468075513839714 minutes
2025-02-13 15:21:23,770:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:23,771:INFO:Initializing create_model()
2025-02-13 15:21:23,771:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:23,771:INFO:Checking exceptions
2025-02-13 15:21:23,771:INFO:Importing libraries
2025-02-13 15:21:23,771:INFO:Copying training dataset
2025-02-13 15:21:23,779:INFO:Defining folds
2025-02-13 15:21:23,779:INFO:Declaring metric variables
2025-02-13 15:21:23,780:INFO:Importing untrained model
2025-02-13 15:21:23,780:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-13 15:21:23,780:INFO:Starting cross validation
2025-02-13 15:21:23,782:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:24,247:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,253:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,319:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,427:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,458:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,524:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,542:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,579:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,584:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,629:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:24,661:INFO:Calculating mean and std
2025-02-13 15:21:24,663:INFO:Creating metrics dataframe
2025-02-13 15:21:24,666:INFO:Uploading results into container
2025-02-13 15:21:24,667:INFO:Uploading model into container now
2025-02-13 15:21:24,667:INFO:_master_model_container: 8
2025-02-13 15:21:24,667:INFO:_display_container: 2
2025-02-13 15:21:24,668:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-13 15:21:24,668:INFO:create_model() successfully completed......................................
2025-02-13 15:21:24,802:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:24,802:INFO:Creating metrics dataframe
2025-02-13 15:21:24,806:INFO:Initializing Ada Boost Classifier
2025-02-13 15:21:24,806:INFO:Total runtime is 0.43194022178649893 minutes
2025-02-13 15:21:24,807:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:24,807:INFO:Initializing create_model()
2025-02-13 15:21:24,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:24,807:INFO:Checking exceptions
2025-02-13 15:21:24,807:INFO:Importing libraries
2025-02-13 15:21:24,808:INFO:Copying training dataset
2025-02-13 15:21:24,818:INFO:Defining folds
2025-02-13 15:21:24,818:INFO:Declaring metric variables
2025-02-13 15:21:24,819:INFO:Importing untrained model
2025-02-13 15:21:24,819:INFO:Ada Boost Classifier Imported successfully
2025-02-13 15:21:24,819:INFO:Starting cross validation
2025-02-13 15:21:24,821:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:25,262:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,276:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,400:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,429:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,564:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,608:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,659:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,717:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,748:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:25,816:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-13 15:21:26,925:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:26,964:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,026:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,114:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,216:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,226:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,274:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,348:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,348:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,350:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:27,389:INFO:Calculating mean and std
2025-02-13 15:21:27,390:INFO:Creating metrics dataframe
2025-02-13 15:21:27,393:INFO:Uploading results into container
2025-02-13 15:21:27,394:INFO:Uploading model into container now
2025-02-13 15:21:27,394:INFO:_master_model_container: 9
2025-02-13 15:21:27,394:INFO:_display_container: 2
2025-02-13 15:21:27,394:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-02-13 15:21:27,395:INFO:create_model() successfully completed......................................
2025-02-13 15:21:27,551:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:27,551:INFO:Creating metrics dataframe
2025-02-13 15:21:27,557:INFO:Initializing Gradient Boosting Classifier
2025-02-13 15:21:27,558:INFO:Total runtime is 0.4778042594591776 minutes
2025-02-13 15:21:27,558:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:27,559:INFO:Initializing create_model()
2025-02-13 15:21:27,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:27,559:INFO:Checking exceptions
2025-02-13 15:21:27,559:INFO:Importing libraries
2025-02-13 15:21:27,559:INFO:Copying training dataset
2025-02-13 15:21:27,576:INFO:Defining folds
2025-02-13 15:21:27,576:INFO:Declaring metric variables
2025-02-13 15:21:27,576:INFO:Importing untrained model
2025-02-13 15:21:27,577:INFO:Gradient Boosting Classifier Imported successfully
2025-02-13 15:21:27,577:INFO:Starting cross validation
2025-02-13 15:21:27,580:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:39,977:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,023:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,218:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,229:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,250:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,349:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,363:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,486:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,517:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,577:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:40,621:INFO:Calculating mean and std
2025-02-13 15:21:40,624:INFO:Creating metrics dataframe
2025-02-13 15:21:40,629:INFO:Uploading results into container
2025-02-13 15:21:40,629:INFO:Uploading model into container now
2025-02-13 15:21:40,630:INFO:_master_model_container: 10
2025-02-13 15:21:40,630:INFO:_display_container: 2
2025-02-13 15:21:40,633:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-13 15:21:40,633:INFO:create_model() successfully completed......................................
2025-02-13 15:21:40,934:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:40,935:INFO:Creating metrics dataframe
2025-02-13 15:21:40,946:INFO:Initializing Linear Discriminant Analysis
2025-02-13 15:21:40,947:INFO:Total runtime is 0.7009660402933756 minutes
2025-02-13 15:21:40,947:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:40,948:INFO:Initializing create_model()
2025-02-13 15:21:40,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:40,949:INFO:Checking exceptions
2025-02-13 15:21:40,949:INFO:Importing libraries
2025-02-13 15:21:40,949:INFO:Copying training dataset
2025-02-13 15:21:40,969:INFO:Defining folds
2025-02-13 15:21:40,969:INFO:Declaring metric variables
2025-02-13 15:21:40,970:INFO:Importing untrained model
2025-02-13 15:21:40,971:INFO:Linear Discriminant Analysis Imported successfully
2025-02-13 15:21:40,972:INFO:Starting cross validation
2025-02-13 15:21:40,974:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:41,741:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,743:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,765:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,853:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,893:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,943:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:41,986:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:42,012:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:42,026:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:42,063:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-13 15:21:42,101:INFO:Calculating mean and std
2025-02-13 15:21:42,102:INFO:Creating metrics dataframe
2025-02-13 15:21:42,106:INFO:Uploading results into container
2025-02-13 15:21:42,107:INFO:Uploading model into container now
2025-02-13 15:21:42,107:INFO:_master_model_container: 11
2025-02-13 15:21:42,107:INFO:_display_container: 2
2025-02-13 15:21:42,108:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-13 15:21:42,108:INFO:create_model() successfully completed......................................
2025-02-13 15:21:42,227:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:42,227:INFO:Creating metrics dataframe
2025-02-13 15:21:42,230:INFO:Initializing Extra Trees Classifier
2025-02-13 15:21:42,230:INFO:Total runtime is 0.7223389426867167 minutes
2025-02-13 15:21:42,230:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:42,231:INFO:Initializing create_model()
2025-02-13 15:21:42,231:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:42,231:INFO:Checking exceptions
2025-02-13 15:21:42,231:INFO:Importing libraries
2025-02-13 15:21:42,231:INFO:Copying training dataset
2025-02-13 15:21:42,239:INFO:Defining folds
2025-02-13 15:21:42,239:INFO:Declaring metric variables
2025-02-13 15:21:42,240:INFO:Importing untrained model
2025-02-13 15:21:42,240:INFO:Extra Trees Classifier Imported successfully
2025-02-13 15:21:42,240:INFO:Starting cross validation
2025-02-13 15:21:42,242:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:45,477:INFO:Calculating mean and std
2025-02-13 15:21:45,481:INFO:Creating metrics dataframe
2025-02-13 15:21:45,485:INFO:Uploading results into container
2025-02-13 15:21:45,487:INFO:Uploading model into container now
2025-02-13 15:21:45,488:INFO:_master_model_container: 12
2025-02-13 15:21:45,488:INFO:_display_container: 2
2025-02-13 15:21:45,490:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-02-13 15:21:45,491:INFO:create_model() successfully completed......................................
2025-02-13 15:21:45,744:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:45,744:INFO:Creating metrics dataframe
2025-02-13 15:21:45,751:INFO:Initializing Light Gradient Boosting Machine
2025-02-13 15:21:45,752:INFO:Total runtime is 0.7810420672098796 minutes
2025-02-13 15:21:45,752:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:45,753:INFO:Initializing create_model()
2025-02-13 15:21:45,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:45,753:INFO:Checking exceptions
2025-02-13 15:21:45,753:INFO:Importing libraries
2025-02-13 15:21:45,754:INFO:Copying training dataset
2025-02-13 15:21:45,772:INFO:Defining folds
2025-02-13 15:21:45,772:INFO:Declaring metric variables
2025-02-13 15:21:45,773:INFO:Importing untrained model
2025-02-13 15:21:45,774:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 15:21:45,774:INFO:Starting cross validation
2025-02-13 15:21:45,776:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:54,653:INFO:Calculating mean and std
2025-02-13 15:21:54,687:INFO:Creating metrics dataframe
2025-02-13 15:21:54,692:INFO:Uploading results into container
2025-02-13 15:21:54,693:INFO:Uploading model into container now
2025-02-13 15:21:54,694:INFO:_master_model_container: 13
2025-02-13 15:21:54,694:INFO:_display_container: 2
2025-02-13 15:21:54,695:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:21:54,695:INFO:create_model() successfully completed......................................
2025-02-13 15:21:54,886:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:54,886:INFO:Creating metrics dataframe
2025-02-13 15:21:54,890:INFO:Initializing Dummy Classifier
2025-02-13 15:21:54,890:INFO:Total runtime is 0.933344022432963 minutes
2025-02-13 15:21:54,890:INFO:SubProcess create_model() called ==================================
2025-02-13 15:21:54,890:INFO:Initializing create_model()
2025-02-13 15:21:54,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C9965DB8B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:54,891:INFO:Checking exceptions
2025-02-13 15:21:54,891:INFO:Importing libraries
2025-02-13 15:21:54,891:INFO:Copying training dataset
2025-02-13 15:21:54,899:INFO:Defining folds
2025-02-13 15:21:54,899:INFO:Declaring metric variables
2025-02-13 15:21:54,900:INFO:Importing untrained model
2025-02-13 15:21:54,900:INFO:Dummy Classifier Imported successfully
2025-02-13 15:21:54,900:INFO:Starting cross validation
2025-02-13 15:21:54,902:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:21:55,355:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,440:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,468:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,568:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,617:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,645:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,646:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,707:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,709:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,731:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-13 15:21:55,755:INFO:Calculating mean and std
2025-02-13 15:21:55,758:INFO:Creating metrics dataframe
2025-02-13 15:21:55,764:INFO:Uploading results into container
2025-02-13 15:21:55,766:INFO:Uploading model into container now
2025-02-13 15:21:55,767:INFO:_master_model_container: 14
2025-02-13 15:21:55,767:INFO:_display_container: 2
2025-02-13 15:21:55,768:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-02-13 15:21:55,768:INFO:create_model() successfully completed......................................
2025-02-13 15:21:55,927:INFO:SubProcess create_model() end ==================================
2025-02-13 15:21:55,927:INFO:Creating metrics dataframe
2025-02-13 15:21:55,935:WARNING:C:\Users\syamc\anaconda3\envs\gen_ai\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-13 15:21:55,937:INFO:Initializing create_model()
2025-02-13 15:21:55,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:21:55,937:INFO:Checking exceptions
2025-02-13 15:21:55,938:INFO:Importing libraries
2025-02-13 15:21:55,938:INFO:Copying training dataset
2025-02-13 15:21:55,946:INFO:Defining folds
2025-02-13 15:21:55,946:INFO:Declaring metric variables
2025-02-13 15:21:55,946:INFO:Importing untrained model
2025-02-13 15:21:55,946:INFO:Declaring custom model
2025-02-13 15:21:55,947:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 15:21:55,949:INFO:Cross validation set to False
2025-02-13 15:21:55,949:INFO:Fitting Model
2025-02-13 15:21:56,100:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.
2025-02-13 15:21:56,100:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-13 15:21:56,100:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-13 15:21:56,101:INFO:[LightGBM] [Info] Total Bins 87
2025-02-13 15:21:56,101:INFO:[LightGBM] [Info] Number of data points in the train set: 41437, number of used features: 1
2025-02-13 15:21:56,102:INFO:[LightGBM] [Info] Start training from score -1.753432
2025-02-13 15:21:56,102:INFO:[LightGBM] [Info] Start training from score -1.196368
2025-02-13 15:21:56,102:INFO:[LightGBM] [Info] Start training from score -1.405421
2025-02-13 15:21:56,102:INFO:[LightGBM] [Info] Start training from score -1.275586
2025-02-13 15:21:56,718:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:21:56,719:INFO:create_model() successfully completed......................................
2025-02-13 15:21:56,891:INFO:_master_model_container: 14
2025-02-13 15:21:56,891:INFO:_display_container: 2
2025-02-13 15:21:56,891:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:21:56,891:INFO:compare_models() successfully completed......................................
2025-02-13 15:21:56,894:INFO:Initializing tune_model()
2025-02-13 15:21:56,894:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>)
2025-02-13 15:21:56,894:INFO:Checking exceptions
2025-02-13 15:21:56,903:INFO:Copying training dataset
2025-02-13 15:21:56,912:INFO:Checking base model
2025-02-13 15:21:56,912:INFO:Base model : Light Gradient Boosting Machine
2025-02-13 15:21:56,913:INFO:Declaring metric variables
2025-02-13 15:21:56,913:INFO:Defining Hyperparameters
2025-02-13 15:21:57,128:INFO:Tuning with n_jobs=-1
2025-02-13 15:21:57,129:INFO:Initializing RandomizedSearchCV
2025-02-13 15:23:17,633:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 1, 'actual_estimator__num_leaves': 90, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 16, 'actual_estimator__learning_rate': 0.4, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 7, 'actual_estimator__bagging_fraction': 0.8}
2025-02-13 15:23:17,633:INFO:Hyperparameter search completed
2025-02-13 15:23:17,633:INFO:SubProcess create_model() called ==================================
2025-02-13 15:23:17,635:INFO:Initializing create_model()
2025-02-13 15:23:17,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C995373A60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 3, 'reg_alpha': 1, 'num_leaves': 90, 'n_estimators': 100, 'min_split_gain': 0.7, 'min_child_samples': 16, 'learning_rate': 0.4, 'feature_fraction': 0.8, 'bagging_freq': 7, 'bagging_fraction': 0.8})
2025-02-13 15:23:17,635:INFO:Checking exceptions
2025-02-13 15:23:17,635:INFO:Importing libraries
2025-02-13 15:23:17,636:INFO:Copying training dataset
2025-02-13 15:23:17,656:INFO:Defining folds
2025-02-13 15:23:17,656:INFO:Declaring metric variables
2025-02-13 15:23:17,657:INFO:Importing untrained model
2025-02-13 15:23:17,657:INFO:Declaring custom model
2025-02-13 15:23:17,660:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 15:23:17,661:INFO:Starting cross validation
2025-02-13 15:23:17,663:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:23:22,411:INFO:Calculating mean and std
2025-02-13 15:23:22,413:INFO:Creating metrics dataframe
2025-02-13 15:23:22,418:INFO:Finalizing model
2025-02-13 15:23:22,677:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2025-02-13 15:23:22,677:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2025-02-13 15:23:22,677:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-02-13 15:23:22,687:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2025-02-13 15:23:22,688:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2025-02-13 15:23:22,688:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2025-02-13 15:23:22,689:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000187 seconds.
2025-02-13 15:23:22,689:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-13 15:23:22,689:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-13 15:23:22,689:INFO:[LightGBM] [Info] Total Bins 87
2025-02-13 15:23:22,689:INFO:[LightGBM] [Info] Number of data points in the train set: 41437, number of used features: 1
2025-02-13 15:23:22,690:INFO:[LightGBM] [Info] Start training from score -1.753432
2025-02-13 15:23:22,691:INFO:[LightGBM] [Info] Start training from score -1.196368
2025-02-13 15:23:22,691:INFO:[LightGBM] [Info] Start training from score -1.405421
2025-02-13 15:23:22,691:INFO:[LightGBM] [Info] Start training from score -1.275586
2025-02-13 15:23:22,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,788:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,791:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,794:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,797:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,799:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,802:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,805:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,808:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,811:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,814:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,817:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,820:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,823:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,826:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,829:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,832:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,834:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,837:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,840:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,843:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,846:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,848:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,851:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,854:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,857:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,860:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,862:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,865:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,868:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,871:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,873:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,876:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,884:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,887:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,906:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,935:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,942:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,950:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,958:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,970:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,972:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,984:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:22,998:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:22,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,010:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,016:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-02-13 15:23:23,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-02-13 15:23:23,037:INFO:Uploading results into container
2025-02-13 15:23:23,039:INFO:Uploading model into container now
2025-02-13 15:23:23,040:INFO:_master_model_container: 15
2025-02-13 15:23:23,040:INFO:_display_container: 3
2025-02-13 15:23:23,041:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=100, n_jobs=-1, num_leaves=90, objective=None,
               random_state=42, reg_alpha=1, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:23:23,041:INFO:create_model() successfully completed......................................
2025-02-13 15:23:23,244:INFO:SubProcess create_model() end ==================================
2025-02-13 15:23:23,244:INFO:choose_better activated
2025-02-13 15:23:23,244:INFO:SubProcess create_model() called ==================================
2025-02-13 15:23:23,245:INFO:Initializing create_model()
2025-02-13 15:23:23,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C99227F4F0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-13 15:23:23,246:INFO:Checking exceptions
2025-02-13 15:23:23,247:INFO:Importing libraries
2025-02-13 15:23:23,247:INFO:Copying training dataset
2025-02-13 15:23:23,259:INFO:Defining folds
2025-02-13 15:23:23,261:INFO:Declaring metric variables
2025-02-13 15:23:23,261:INFO:Importing untrained model
2025-02-13 15:23:23,261:INFO:Declaring custom model
2025-02-13 15:23:23,262:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-13 15:23:23,263:INFO:Starting cross validation
2025-02-13 15:23:23,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-13 15:23:30,156:INFO:Calculating mean and std
2025-02-13 15:23:30,157:INFO:Creating metrics dataframe
2025-02-13 15:23:30,161:INFO:Finalizing model
2025-02-13 15:23:30,335:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.
2025-02-13 15:23:30,335:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-02-13 15:23:30,335:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-02-13 15:23:30,336:INFO:[LightGBM] [Info] Total Bins 87
2025-02-13 15:23:30,336:INFO:[LightGBM] [Info] Number of data points in the train set: 41437, number of used features: 1
2025-02-13 15:23:30,337:INFO:[LightGBM] [Info] Start training from score -1.753432
2025-02-13 15:23:30,337:INFO:[LightGBM] [Info] Start training from score -1.196368
2025-02-13 15:23:30,337:INFO:[LightGBM] [Info] Start training from score -1.405421
2025-02-13 15:23:30,337:INFO:[LightGBM] [Info] Start training from score -1.275586
2025-02-13 15:23:31,145:INFO:Uploading results into container
2025-02-13 15:23:31,146:INFO:Uploading model into container now
2025-02-13 15:23:31,147:INFO:_master_model_container: 16
2025-02-13 15:23:31,147:INFO:_display_container: 4
2025-02-13 15:23:31,148:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:23:31,149:INFO:create_model() successfully completed......................................
2025-02-13 15:23:31,308:INFO:SubProcess create_model() end ==================================
2025-02-13 15:23:31,309:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.396
2025-02-13 15:23:31,310:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=100, n_jobs=-1, num_leaves=90, objective=None,
               random_state=42, reg_alpha=1, reg_lambda=3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.3953
2025-02-13 15:23:31,310:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2025-02-13 15:23:31,310:INFO:choose_better completed
2025-02-13 15:23:31,311:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2025-02-13 15:23:31,325:INFO:_master_model_container: 16
2025-02-13 15:23:31,326:INFO:_display_container: 3
2025-02-13 15:23:31,327:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-13 15:23:31,327:INFO:tune_model() successfully completed......................................
2025-02-13 15:28:27,200:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:28:27,201:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:28:27,201:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:28:27,202:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-13 15:29:10,167:INFO:PyCaret ClassificationExperiment
2025-02-13 15:29:10,167:INFO:Logging name: clf-default-name
2025-02-13 15:29:10,167:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-13 15:29:10,167:INFO:version 3.3.2
2025-02-13 15:29:10,167:INFO:Initializing setup()
2025-02-13 15:29:10,167:INFO:self.USI: 0147
2025-02-13 15:29:10,167:INFO:self._variable_keys: {'X', 'n_jobs_param', 'is_multiclass', 'y_train', 'fold_shuffle_param', 'fold_generator', 'exp_id', 'log_plots_param', 'X_test', 'logging_param', 'idx', 'data', 'exp_name_log', 'seed', '_ml_usecase', '_available_plots', 'target_param', 'memory', 'y', 'html_param', 'y_test', 'USI', 'X_train', 'fix_imbalance', 'gpu_param', 'gpu_n_jobs_param', 'fold_groups_param', 'pipeline'}
2025-02-13 15:29:10,167:INFO:Checking environment
2025-02-13 15:29:10,167:INFO:python_version: 3.10.15
2025-02-13 15:29:10,168:INFO:python_build: ('main', 'Oct  3 2024 07:22:19')
2025-02-13 15:29:10,168:INFO:machine: AMD64
2025-02-13 15:29:10,183:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-13 15:29:10,187:INFO:Memory: svmem(total=17008857088, available=7492407296, percent=55.9, used=9516449792, free=7492407296)
2025-02-13 15:29:10,187:INFO:Physical Core: 6
2025-02-13 15:29:10,187:INFO:Logical Core: 12
2025-02-13 15:29:10,187:INFO:Checking libraries
2025-02-13 15:29:10,188:INFO:System:
2025-02-13 15:29:10,188:INFO:    python: 3.10.15 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:19) [MSC v.1929 64 bit (AMD64)]
2025-02-13 15:29:10,188:INFO:executable: C:\Users\syamc\anaconda3\envs\gen_ai\python.exe
2025-02-13 15:29:10,188:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-13 15:29:10,188:INFO:PyCaret required dependencies:
2025-02-13 15:29:10,233:INFO:                 pip: 24.2
2025-02-13 15:29:10,233:INFO:          setuptools: 75.1.0
2025-02-13 15:29:10,233:INFO:             pycaret: 3.3.2
2025-02-13 15:29:10,233:INFO:             IPython: 8.28.0
2025-02-13 15:29:10,233:INFO:          ipywidgets: 8.1.5
2025-02-13 15:29:10,233:INFO:                tqdm: 4.66.5
2025-02-13 15:29:10,233:INFO:               numpy: 1.26.4
2025-02-13 15:29:10,233:INFO:              pandas: 2.1.4
2025-02-13 15:29:10,234:INFO:              jinja2: 3.1.4
2025-02-13 15:29:10,234:INFO:               scipy: 1.11.4
2025-02-13 15:29:10,234:INFO:              joblib: 1.3.2
2025-02-13 15:29:10,234:INFO:             sklearn: 1.4.2
2025-02-13 15:29:10,234:INFO:                pyod: 2.0.3
2025-02-13 15:29:10,234:INFO:            imblearn: 0.13.0
2025-02-13 15:29:10,234:INFO:   category_encoders: 2.7.0
2025-02-13 15:29:10,234:INFO:            lightgbm: 4.5.0
2025-02-13 15:29:10,234:INFO:               numba: 0.61.0
2025-02-13 15:29:10,234:INFO:            requests: 2.32.3
2025-02-13 15:29:10,234:INFO:          matplotlib: 3.7.5
2025-02-13 15:29:10,234:INFO:          scikitplot: 0.3.7
2025-02-13 15:29:10,235:INFO:         yellowbrick: 1.5
2025-02-13 15:29:10,235:INFO:              plotly: 5.24.1
2025-02-13 15:29:10,235:INFO:    plotly-resampler: Not installed
2025-02-13 15:29:10,235:INFO:             kaleido: 0.2.1
2025-02-13 15:29:10,235:INFO:           schemdraw: 0.15
2025-02-13 15:29:10,235:INFO:         statsmodels: 0.14.4
2025-02-13 15:29:10,235:INFO:              sktime: 0.26.0
2025-02-13 15:29:10,235:INFO:               tbats: 1.1.3
2025-02-13 15:29:10,235:INFO:            pmdarima: 2.0.4
2025-02-13 15:29:10,235:INFO:              psutil: 6.0.0
2025-02-13 15:29:10,235:INFO:          markupsafe: 3.0.1
2025-02-13 15:29:10,235:INFO:             pickle5: Not installed
2025-02-13 15:29:10,236:INFO:         cloudpickle: 3.1.1
2025-02-13 15:29:10,236:INFO:         deprecation: 2.1.0
2025-02-13 15:29:10,236:INFO:              xxhash: 3.5.0
2025-02-13 15:29:10,236:INFO:           wurlitzer: Not installed
2025-02-13 15:29:10,236:INFO:PyCaret optional dependencies:
2025-02-13 15:29:11,907:INFO:                shap: Not installed
2025-02-13 15:29:11,907:INFO:           interpret: Not installed
2025-02-13 15:29:11,908:INFO:                umap: Not installed
2025-02-13 15:29:11,908:INFO:     ydata_profiling: Not installed
2025-02-13 15:29:11,908:INFO:  explainerdashboard: Not installed
2025-02-13 15:29:11,908:INFO:             autoviz: Not installed
2025-02-13 15:29:11,908:INFO:           fairlearn: Not installed
2025-02-13 15:29:11,909:INFO:          deepchecks: Not installed
2025-02-13 15:29:11,909:INFO:             xgboost: Not installed
2025-02-13 15:29:11,909:INFO:            catboost: Not installed
2025-02-13 15:29:11,909:INFO:              kmodes: Not installed
2025-02-13 15:29:11,910:INFO:             mlxtend: Not installed
2025-02-13 15:29:11,910:INFO:       statsforecast: Not installed
2025-02-13 15:29:11,910:INFO:        tune_sklearn: Not installed
2025-02-13 15:29:11,910:INFO:                 ray: Not installed
2025-02-13 15:29:11,911:INFO:            hyperopt: Not installed
2025-02-13 15:29:11,911:INFO:              optuna: Not installed
2025-02-13 15:29:11,911:INFO:               skopt: Not installed
2025-02-13 15:29:11,911:INFO:              mlflow: Not installed
2025-02-13 15:29:11,911:INFO:              gradio: Not installed
2025-02-13 15:29:11,911:INFO:             fastapi: 0.115.0
2025-02-13 15:29:11,911:INFO:             uvicorn: 0.31.1
2025-02-13 15:29:11,912:INFO:              m2cgen: Not installed
2025-02-13 15:29:11,912:INFO:           evidently: Not installed
2025-02-13 15:29:11,912:INFO:               fugue: Not installed
2025-02-13 15:29:11,912:INFO:           streamlit: Not installed
2025-02-13 15:29:11,912:INFO:             prophet: Not installed
2025-02-13 15:29:11,913:INFO:None
2025-02-13 15:29:11,913:INFO:Set up data.
2025-02-13 15:29:12,441:INFO:Set up folding strategy.
2025-02-13 15:29:12,442:INFO:Set up train/test split.
2025-02-13 15:29:12,613:INFO:Set up index.
2025-02-13 15:29:12,617:INFO:Assigning column types.
2025-02-13 15:29:12,625:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-13 15:29:12,864:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 15:29:12,880:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:29:13,078:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,079:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,266:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-13 15:29:13,269:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:29:13,392:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,393:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,394:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-13 15:29:13,623:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:29:13,781:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,782:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:13,984:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-13 15:29:14,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,068:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-13 15:29:14,299:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,300:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,547:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,547:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-02-13 15:29:14,552:INFO:Preparing preprocessing pipeline...
2025-02-13 15:29:14,555:INFO:Set up simple imputation.
2025-02-13 15:29:14,565:INFO:Set up encoding of categorical features.
2025-02-13 15:29:15,396:INFO:Finished creating preprocessing pipeline.
2025-02-13 15:29:15,416:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\syamc\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=['...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Comments'],
                                    transformer=TargetEncoder(cols=['Comments'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-13 15:29:15,416:INFO:Creating final display dataframe.
